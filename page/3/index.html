<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-BERT模型应用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/19/BERT%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/" class="article-date">
  <time datetime="2020-08-19T05:38:05.000Z" itemprop="datePublished">2020-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/19/BERT%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/">BERT模型应用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/19/BERT%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/" data-id="ckhnlm6260001nng7h4xrerhv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bert/" rel="tag">bert</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Python中常用模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/12/Python%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/" class="article-date">
  <time datetime="2020-08-12T09:34:08.000Z" itemprop="datePublished">2020-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/12/Python%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/">Python中常用模块</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0-模块列表"><a href="#0-模块列表" class="headerlink" title="0.模块列表"></a>0.模块列表</h1><p>Seaborn</p>
<h1 id="1-Seaborn"><a href="#1-Seaborn" class="headerlink" title="1.Seaborn"></a>1.Seaborn</h1><p>seaborn是在matplotlib基础上进行封装，Seaborn就是让困难的东西更加简单。用Matplotlib最大的困难是其默认的各种参数，而Seaborn则完全避免了这一问题。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/12/Python%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/" data-id="ckhnlm62t0019nng721g29i2s" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-BERT模型分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/10/BERT%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/" class="article-date">
  <time datetime="2020-08-10T02:05:49.000Z" itemprop="datePublished">2020-08-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/10/BERT%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/">BERT模型分析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h1><p>Transformer和Bert之间的关系:<br>Transformer模型是2017年谷歌发表的论文attention is all you need中提出的seq2seq模型。现在已经取得了大范围的应用和扩展，而BERT就是从transformer中衍生出来的预训练语言模型</p>
<p>BERT使用TensorFlow，2018年提供了BERT的PyTorch版本Transformer</p>
<p>来源:<br>论文:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>模型的整体框架就是一个seq2seq结构<br>Google AI Language<br>NLP中的预训练:<br>词嵌入是NLP深度学习的基础</p>
<p><img src="%E8%AF%8D%E5%90%91%E9%87%8F.png"><br>词嵌入(word2vec,GloVe)是通过共现文本对统计，对文本语料库进行预训练<br><img src="%E5%85%B1%E7%8E%B0%E6%96%87%E6%9C%AC%E5%AF%B9%E7%BB%9F%E8%AE%A1.png"></p>
<p>上下文表示:<br>之前的问题:<br>词嵌入是应用于与上下文无关的方式<br><img src="%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E7%9A%84%E8%AF%8D%E5%B5%8C%E5%85%A5.png"></p>
<p>解决方法:<br>在语料库中训练上下文表示。<br><img src="%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9B%B8%E5%85%B3%E7%9A%84%E8%AF%8D%E5%B5%8C%E5%85%A5.png"></p>
<p>上下文相关的表示方法:<br>半监督的序列学习,Google,2015<br><img src="%E5%8D%8A%E7%9B%91%E7%9D%A3%E7%9A%84%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0.png"></p>
<p>ELMo:深度上下文词嵌入，2017<br><img src="%E6%B7%B1%E5%BA%A6%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5.png"></p>
<p>通过通用的预训练来提升语言理解OpenAL,2018<br><img src="%E9%80%9A%E8%BF%87%E9%80%9A%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%9D%A5%E6%8F%90%E5%8D%87%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3.png"></p>
<p>模型:借鉴于Attention Is All You Need中的Encoder<br><img src="AttentionIsAllYouNeed.png"><br>完全由注意力机制构成，其分为左右两部分:<br>左侧:编码器部分<br>N=6<br>All Layers output size 512<br>Embedding<br>Positional Encoding<br><img src="PositionalEncoding.png"><br>Notice the Residual connection<br>Multi-head Attention<br>LayerNorm(x + Sublayer(x))<br>Position wise feed forward</p>
<p>右侧:解码器部分<br>N=6<br>All Layers output size 512<br>Embedding<br>Positional Encoding<br>Notice the Residual connection<br>Multi-head Attention<br><img src="Multi-headAttention.png"><br>LayerNorm(x + Sublayer(x))<br>Position wise feed forward<br>Softmax<br><img src="SoftMax.png"></p>
<p>其中，解码器中的Multi-head Attention的原理:<br><img src="Multi-HeadAttention%E6%A8%A1%E5%9E%8B.png"><br>其中点乘注意力模型为:<br><img src="ScaledDotProductAttention.png"><br>公式:<br><img src="Attention%E6%A8%A1%E5%9E%8B.png"><br>多注意力模型公式:<br><img src="%E5%A4%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%E5%85%AC%E5%BC%8F.png"><br>其中，公式中的Q，K，V的定义为:<br>编码器-解码器注意力层中，序列Q来自之前的解码器层，记忆力关键词K和数值V来自编码器的输出。<br>这三个变量都来自之前的层(hidden state)</p>
<p>然后是Feed Forward前馈网络Position-wise Feed-Forward network<br><img src="PositionWiseFeedForwardNetwork.png"></p>
<p>Attention Is All You Need解析:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b1030350aadb">https://www.jianshu.com/p/b1030350aadb</a><br>Seq2Seq模型的简介:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b2b95f945a98">https://www.jianshu.com/p/b2b95f945a98</a><br>Seq2Seq模型是输出的长度不确定时采用的模型，这种情况一般是在机器翻译的任务中出现，将一句中文翻译成英文，那么这句英文的长度有可能会比中文短，也有可能会比中文长，所以输出的长度就不确定了。如下图所，输入的中文长度为4，输出的英文长度为2。<br><img src="Seq2Seq%E6%A8%A1%E5%9E%8B.webp"><br>在网络结构中，输入一个中文序列，然后输出它对应的中文翻译，输出的部分的结果预测后面，根据上面的例子，也就是先输出“machine”，将”machine”作为下一次的输入，接着输出”learning”,这样就能输出任意长的序列。<br>机器翻译、人机对话、聊天机器人等等，这些都是应用在当今社会都或多或少的运用到了我们这里所说的Seq2Seq。</p>
<p>Seq2Seq结构<br>seq2seq属于encoder-decoder结构的一种，这里看看常见的encoder-decoder结构，基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder。encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量C。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。<br><img src="encoder_decoder%E7%BB%93%E6%9E%84.webp"><br>而decoder则负责根据语义向量生成指定的序列，这个过程也称为解码，如下图，最简单的方式是将encoder得到的语义变量作为初始状态输入到decoder的RNN中，得到输出序列。可以看到上一时刻的输出会作为当前时刻的输入，而且其中语义向量C只作为初始状态参与运算，后面的运算都与语义向量C无关。<br><img src="encoder_decoder%E7%BB%93%E6%9E%841.webp"><br>decoder处理方式还有一种，就是语义向量参与了序列的所有时刻的运算，上一时刻的输出仍然作为当前时刻的输入，但是语义向量会参与到所有时刻的运算。<br><img src="encoder_decoder%E7%BB%93%E6%9E%842.webp"></p>
<p>如何训练Seq2Seq模型:<br>RNN是可以学习概率分布，然后进行预测，比如我们输入t时刻的数据后，预测t+1时刻的数据，比较常见的是字符预测例子或者时间序列预测。为了得到概率分布，一般会在RNN的输出层使用softmax激活函数，就可以得到每个分类的概率。<br>Softmax在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类（C &gt; 2）问题，分类器最后的输出单元需要Softmax 函数进行数值处理。关于Softmax 函数的定义如下所示：<br><img src="softmax.svg"></p>
<p>其中，Vi是分类器前级输出单元的输出。i表示类别索引，总的类别个数为C,表示的是当前元素的指数与所有元素指数和的比值。Softmax 将多分类的输出数值转化为相对概率，更容易理解和比较。我们来看下面这个例子。<br>一个多分类问题，C = 4。线性分类器模型最后输出层包含了四个输出值，分别是：<br><img src="math%E8%BE%93%E5%85%A5.svg"><br>通过Softmax处理之后，数值转化为相对概率:<br><img src="math%E8%BE%93%E5%87%BA.svg"><br>很明显，Softmax 的输出表征了不同类别之间的相对概率。我们可以清晰地看出，S1 = 0.8390，对应的概率最大，则更清晰地可以判断预测为第1类的可能性更大。Softmax 将连续数值转化成相对概率，更有利于我们理解。<br><img src="Seq2Seq%E6%A8%A1%E5%9E%8BSoftmax%E5%85%AC%E5%BC%8F.webp"><br>对于RNN，对于某个序列，对于时序t，它的词向量输出概率为P(xt|x1,x2,…,xt-1)，则softmax层每个神经元的计算如下。<br><img src="%E7%A5%9E%E7%BB%8F%E5%85%83%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.svg"><br>其中ht是隐含状态，它与上一时刻的状态及当前输入有关，即h<sub>t</sub>=f(h<sub>t-1</sub>,x<sub>t</sub>)<br>那么整个序列的概率就为:<br><img src="%E5%BA%8F%E5%88%97%E7%9A%84%E6%A6%82%E7%8E%87.svg"><br><img src="RNNDecodeEncode%E6%A8%A1%E5%9E%8B.webp"></p>
<p>而对于encoder-decoder模型，设有输入序列x<sub>1</sub>,x<sub>2</sub>，x<sub>3</sub>,…,x<sub>T</sub>,输出序列y<sub>1</sub>，y<sub>2</sub>,y<sub>3</sub>，…,y<sub>T</sub>，输入序列和输出序列的长度可能不同。那么其实就需要根据输入序列去得到输出序列可能输出的词概率，于是有下面的条件概率，￼发生的情况下，￼发生的概率等于￼连乘，如下公式所示。其中v表示￼对应的隐含状态向量，它其实可以等同表示输入序列。</p>
<p>Seq2Seq模型:<a target="_blank" rel="noopener" href="https://dataxujing.github.io/seq2seqlearn/chapter2/">https://dataxujing.github.io/seq2seqlearn/chapter2/</a></p>
<p>部分教程:<a target="_blank" rel="noopener" href="https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/">https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/</a><br>bert tensorflow版本:<a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a><br>bert pytorch版本:<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a><br>bert pytorch版本说明文档:<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/quicktour.html">https://huggingface.co/transformers/quicktour.html</a><br>BERT Bidirectional Encoder Representations from Transformers<br>一种从Transformers模型得来的双向编码表征模型<br>下面通过理论和实践两部分来解释BERT模型<br>1.理论：<br>1.1 为什么需要BERT<br>1.2 它背后的核心思想是什么<br>1.3 它是如何工作的<br>1.4 我们什么时候可以使用它并且对其进行微调</p>
<p>2.实践</p>
<h1 id="1-理论"><a href="#1-理论" class="headerlink" title="1.理论"></a>1.理论</h1><h2 id="1-1-为什么需要BERT"><a href="#1-1-为什么需要BERT" class="headerlink" title="1.1 为什么需要BERT"></a>1.1 为什么需要BERT</h2><p>NLP的最大挑战之一是缺乏足够的培训数据。总体而言，有大量文本数据可用，但是如果我们要创建特定于任务的数据集，则需要将该堆拆分为许多不同的类型。而当我们这样做时，我们最终仅得到数千或数十万个人工标记的训练样本。但是，为了表现良好，基于深度学习的NLP模型需要大量数据-在数百万或数十亿的带注释的训练样本上进行训练时，他们看到了重大改进。为了解决上面的数据问题，研究人员开发了多种技术，可在网络上使用大量未注释的文本来训练通用语言表示模型（这被称为预训练）。然后，可以在较小的用于特定任务的数据集上微调这些通用的预训练模型，例如，在处理诸如问题回答和情感分析之类的问题。与从头开始对较小的特定于任务的数据集进行训练相比，此方法可显着提高准确性。BERT是用于NLP预训练的最新的技术。它在深度学习社区引起了轰动，因为它在各种NLP任务（例如问题解答）中都提供了最先进的结果。</p>
<p>关于BERT的最好的部分是可以免费下载和使用-我们可以使用BERT模型从文本数据中提取高质量的语言特征，也可以根据特定任务（例如情感分析）微调这些模型并使用我们自己的数据回答问题，以产生最先进的预测。</p>
<h2 id="1-2-它背后的核心思想是什么"><a href="#1-2-它背后的核心思想是什么" class="headerlink" title="1.2 它背后的核心思想是什么"></a>1.2 它背后的核心思想是什么</h2><p>语言建模的真正意义是什么？语言模型试图解决哪些问题？基本上，他们的任务是根据上下文“填补空白”。例如，给定</p>
<p>“那个女人去商店买了_____鞋。”<br>‘The woman went to the store and bought a __ of shoes’</p>
<p>语言模型可能会说20%是’cart’，80%是’pair’</p>
<p>在BERT模型出现之前，语言模型会在训练期间从左到右或这将从左到右和从右到左的结合来查看此文本序列。这种单向方法很适合生成句子-我们可以预测下一个单词，将其附加到序列中，然后预测下一个单词的下一个单词，直到获得完整的句子。</p>
<p>现在使用BERT模型，这是一种经过双向训练的语言模型（这也是其关键的技术创新）。这意味着与单向语言模型相比，我们现在可以对语言上下文和流程有更深刻的了解。</p>
<p>BERT并没有预测序列中的下一个单词，而是使用一种称为Masked LM（MLM）的新颖技术：它 随机屏蔽句子中的单词，然后尝试预测它们。掩蔽Mask意味着该模型从两个方向看，并且它使用句子的整个上下文（包括左边和右边的语境）来预测被掩盖的单词。与以前的语言模型不同，它同时考虑了前一个和下一个tokens令牌  。LSTM模型将从左到右和从右到左相结合，并没有同时考虑这两个方向。（不过，说BERT是无方向性的可能更准确。）</p>
<p>额外提出一个问题，为什么这个无方向性的方法这么有效呢？<br>预训练语言表示分为context-free(和上下文无关)和context-based(基于上下文).<br>context-based表示可以是单向或者双向。<br>context-free表示,例如word2vec产生了一个简单的词嵌入(数字向量)表示词汇表中的每一个词语。<br>例如bank将会有相同的context-free表示在’bank account’和’bank of river’中。</p>
<p>另一方面，基于上下文的模型建立代表每一个词语的表示是基于句子中的其他词语。<br>例如:I accessed the bank account<br>在单向网络模型中，词语bank是基于I accessed the bank 而不是 account<br>但是，在BERT中，bank是使用了其上下文 I accessed the … account<br>它从深度神经网络的最底层开始，使其深度双向化。</p>
<p>而且，BERT是基于Transformer model architecture(Transformer模型架构),而不是LSTM，我们很快就会讨论BERT的细节，但是总体而言:<br>Transformer通过执行少量恒定的步骤来工作。在每个步骤中，它都会应用一种注意力机制来理解句子中所有单词之间的关系，而不管它们各自的位置如何。例如，给定句子’I arrive at the bank after crossing the river’，以确定‘bank’一词是指河岸而不是金融机构，那么Transformer可以学会立即注意单词“ river”，然后一步一步做出决定。</p>
<p>现在，我们了解了BERT的关键思想，让我们深入研究细节。</p>
<h2 id="1-3-它是如何工作的"><a href="#1-3-它是如何工作的" class="headerlink" title="1.3 它是如何工作的"></a>1.3 它是如何工作的</h2><p>BERT依赖于Transformer（一种学习文本中单词之间的上下文关系的注意力机制）。一个基本的Transformer包括一个读取文本输入的编码器和一个对任务进行预测的解码器。由于BERT的目标是生成语言表示模型，因此只需要编码器部分即可。BERT编码器的输入是一系列令牌，这些令牌首先被转换为矢量，然后在神经网络中进行处理。但是在开始处理之前，BERT需要对输入进行处理并用一些额外的元数据修饰：<br>令牌嵌入：在第一个句子的开头将[CLS]令牌添加到输入的单词令牌中，并在每个句子的末尾插入[SEP]令牌。<br>段嵌入：将指示句子A或句子B的标记添加到每个标记。这允许编码器区分句子。<br>位置嵌入：将位置嵌入添加到每个标记，以指示其在句子中的位置。<br><img src="BERT%E8%BE%93%E5%85%A5.png"><br>BERT输入表示：输入嵌入是令牌嵌入、分段嵌入和位置嵌入的总和</p>
<p>本质上，Transformer会堆叠一个将序列映射到序列的层，因此输出也是对应索引的输入和输出令牌之间一对一对应关系的向量序列。而且正如我们之前所了解的，BERT不会尝试预测句子中的下一个单词。培训使用以下两种策略：</p>
<h3 id="1-3-1-Masked-LM-MLM"><a href="#1-3-1-Masked-LM-MLM" class="headerlink" title="1.3.1 Masked LM (MLM)"></a>1.3.1 Masked LM (MLM)</h3><p>这里的想法很简单：随机掩盖输入中15％的单词（用[MASK]令牌替换），通过基于BERT注意的编码器运行整个序列，然后根据序列中其他未屏蔽单词提供的上下文来预测被掩盖的词语。但是，这种掩盖方法存在一个问题-模型仅在输入中存在[MASK]令牌的时候尝试预测，而我们希望模型在不管输入中有什么令牌的时候，都尝试预测正确的令牌。输入。要解决此问题，15％用于屏蔽的令牌中：</p>
<p>实际上有80％的令牌已替换为令牌[MASK]。<br>10％的令牌被替换为随机令牌。<br>10％的令牌保持不变。<br>在训练BERT损失函数时，仅考虑掩蔽标记的预测，而忽略非掩蔽标记的预测。这导致模型收敛的速度比从左至右或从右至左的模型慢得多。</p>
<h3 id="1-3-2-Next-Sentence-Prediction-NSP"><a href="#1-3-2-Next-Sentence-Prediction-NSP" class="headerlink" title="1.3.2 Next Sentence Prediction(NSP)"></a>1.3.2 Next Sentence Prediction(NSP)</h3><p>为了理解两个句子之间的关系，BERT训练过程还使用下一个句子预测。具有这种理解的预训练模型与诸如回答问题之类的任务有关。在训练过程中，该模型接受句子的输入对，并且学习预测第二个句子是否也是原始文本中的下一个句子。</p>
<p>如前所述，BERT使用特殊的[SEP]标记分隔句子。在训练过程中，模型每次输入两个输入语句，从而：</p>
<p>第二句话是第一句话之后的概率是50％。<br>有50％的概率是整个语料库中的随机句子。</p>
<p>然后要求BERT预测第二个句子是否是随机的，并假设该随机句子将与第一个句子断开：<br><img src="BERT%E5%8F%A5%E5%AD%90%E9%A2%84%E6%B5%8B.png"></p>
<p>为了预测第二句话是否与第一句话相连，基本上整个输入序列都会经过基于Transformer的模型，使用简单的分类层将[CLS]令牌的输出转换为2×1形状的矢量，并使用softmax确认IsNext-Label标签。</p>
<p>结合使用“Marked LM”和“下一句预测”对模型进行训练。这是为了最小化这两种策略的组合损失函数-  “在一起更好”。</p>
<h3 id="1-3-4-结构"><a href="#1-3-4-结构" class="headerlink" title="1.3.4 结构"></a>1.3.4 结构</h3><p>有4种类型的BERT预训练版本，取决于模型结构的大小<br>BERT-Base：12层，768个隐藏节点，12个注意点，110M参数<br>BERT-Large：24层，1024个隐藏节点，16个注意点，340M参数</p>
<p>有趣的事实：BERT-Base在4个云TPU上进行了4天的训练，而BERT-Large在16个TPU上进行了4天的训练！</p>
<h2 id="1-4-我们什么时候可以使用它并且对其进行微调"><a href="#1-4-我们什么时候可以使用它并且对其进行微调" class="headerlink" title="1.4 我们什么时候可以使用它并且对其进行微调"></a>1.4 我们什么时候可以使用它并且对其进行微调</h2><p>在一般语言理解（例如自然语言推理，情感分析，问题回答，释义检测和语言可接受性）下，BERT在各种任务上的表现都超过了最新技术。</p>
<p>现在，我们如何针对特定任务对其进行微调？BERT可用于多种语言任务。如果要基于自己的数据集微调原始模型，可以通过在核心模型的顶部添加一个单独的层来实现。</p>
<p>例如，假设我们正在创建一个问答应用程序。本质上，问题解答只是一项预测任务-在收到问题作为输入时，应用程序的目标是从某个语料库中识别正确的答案。因此，给定一个问题和一个上下文段落，该模型将从最有可能回答该问题的段落中预测一个开始和结束标记。这意味着使用BERT可以通过学习两个标记答案开头和结尾的额外矢量来训练我们应用程序的模型。<br><img src="BERT%E5%9B%9E%E7%AD%94.png"></p>
<p>就像句子对任务一样，问题将成为输入序列中的第一个句子，第二个句子成为段落。但是，这一次在微调期间学习了两个新参数：起始向量和结束向量。</p>
<p>在微调训练中，大多数超参数保持与BERT训练中相同；本文针对需要调整的超参数给出了具体指导。</p>
<p>请注意，如果要进行微调，我们需要将输入转换为用于预训练核心BERT模型的特定格式，例如，我们需要添加特殊标记以标记开始（[CLS ]）以及句子的分隔/结尾（[SEP]）和用于区分不同句子的句段ID －将数据转换为BERT使用的特征。</p>
<h1 id="2-实践"><a href="#2-实践" class="headerlink" title="2.实践"></a>2.实践</h1><p>实践环境:ubuntu python torch transformers</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/10/BERT%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/" data-id="ckhnlm65o009gnng70qbw9w5r" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-anaconda教程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/07/anaconda%E6%95%99%E7%A8%8B/" class="article-date">
  <time datetime="2020-08-07T06:13:50.000Z" itemprop="datePublished">2020-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/07/anaconda%E6%95%99%E7%A8%8B/">anaconda教程</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h1><p>Anaconda是开源的包、环境管理器，里面包含大量安装好的工具包.最主要的是能够通过它进行不同版本的python环境的安装和切换。<br>例如tensorflow模块经常更新，很多程序都是在1.xx版本，最新的2.xx版本的资料不够详细。所以，我们在不同版本的tensorflow的时候，我们就可以直接在tensorflow中运行程序。<br>但是觉得挺麻烦的，在Ubuntu环境下可以直接下载两个版本的Python，直接使用。</p>
<p>下载anaconda:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget xxx <span class="comment">#从官网下载，xxx为连接</span></span><br><span class="line">bash xxx.sh</span><br></pre></td></tr></table></figure>
<p>注意里面的选项，最后一个是yes</p>
<p>在ubuntu上安装了Anaconda之后，每次启动终端就会自动进入conda的base环境，可以通过如下指令退出conda环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<p>但是考虑到每次都需要执行这一步才能退出，比较麻烦，因此想要启动终端后不进入conda环境。网上推荐通过修改conda的config文件来实现：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="built_in">set</span> auto_activate_base <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h1 id="1-Anaconda环境切换及使用："><a href="#1-Anaconda环境切换及使用：" class="headerlink" title="1.Anaconda环境切换及使用："></a>1.Anaconda环境切换及使用：</h1><p>查看当前的环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda info -e</span><br><span class="line">python -V</span><br></pre></td></tr></table></figure>

<p>新建环境，选择python版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conda create --name torch0.3 python=3.5</span><br><span class="line">其中 torch0.3是新环境的名字，python=3.5是选择的python版本</span><br><span class="line"><span class="comment">#conda create --name python32 --clone python321</span></span><br><span class="line"><span class="comment">#conda remove --name old_name --all </span></span><br><span class="line"></span><br><span class="line">激活切换环境</span><br><span class="line">conda activate torch0.3</span><br><span class="line"></span><br><span class="line">回到之前的环境</span><br><span class="line">conda deactivate</span><br><span class="line"></span><br><span class="line">删除一个已有的环境</span><br><span class="line">conda remove --name torch0.3 --all</span><br></pre></td></tr></table></figure>

<p>我需要一个pytorch0.3版本，但是官网上已经没有wins下的该版本，所以直接使用anaconda官网下的链接进行下载</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c peterjc123 pytorch</span><br></pre></td></tr></table></figure>

<p>我们建立了新的环境，但是spyder依旧是过去的怎么版。安装新spyder，然后打开spyder</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install spyder</span><br><span class="line">spyder</span><br></pre></td></tr></table></figure>

<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ:"></a>FAQ:</h1><h2 id="在Spyder中如何弹出窗口动态显示图片："><a href="#在Spyder中如何弹出窗口动态显示图片：" class="headerlink" title="在Spyder中如何弹出窗口动态显示图片："></a>在Spyder中如何弹出窗口动态显示图片：</h2><p>Tools–&gt;Preferences–&gt;IPython console–&gt;Graphics–&gt;Graphics backend–&gt; Backend–&gt;设置成Automatic</p>
<h2 id="当下载超时的时候该怎么办？"><a href="#当下载超时的时候该怎么办？" class="headerlink" title="当下载超时的时候该怎么办？"></a>当下载超时的时候该怎么办？</h2><p>当国内没有镜像的时候直接在Anaconda Cload上搜索<br>修改.condarc文件，切换国内源</p>
<p>conda config –show-sources</p>
<p>恢复默认源<br>conda config –remove-key channels</p>
<p>其中只有default源，我们可以通过<br>conda config –add channels <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>来添加镜像</p>
<p>显示通道地址<br>conda config –set show_channel_urls yes</p>
<p>或者直接在windows：C:\users\username\，linux：/home/username/下的.condarc文件中修改<br>channels:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a></li>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/</a></li>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</a></li>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</a></li>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</a></li>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a></li>
<li>defaults<br>ssl_verify: true</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/07/anaconda%E6%95%99%E7%A8%8B/" data-id="ckhnlm63z003hnng7665351s4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/anaconda/" rel="tag">anaconda</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-GraphSAGE" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/02/GraphSAGE/" class="article-date">
  <time datetime="2020-08-02T09:06:45.000Z" itemprop="datePublished">2020-08-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/02/GraphSAGE/">GraphSAGE</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>GCN是一种在图中结合拓扑结构和顶点属性信息学习顶点的embedding表示的方法。然而GCN要求在一个确定的图中去学习顶点的embedding，无法直接泛化到在训练过程没有出现过的顶点，即属于一种直推式(transductive)的学习。</p>
<p>本文介绍的GraphSAGE则是一种能够利用顶点的属性信息高效产生未知顶点embedding的一种归纳式(inductive)学习的框架。</p>
<p>其核心思想是通过学习一个对邻居顶点进行聚合表示的函数来产生目标顶点的embedding向量。</p>
<p><img src="%E5%9B%BE%E7%A4%BA1.png"></p>
<p>GraphSAGE 是Graph SAmple and aggreGatE的缩写，其运行流程如上图所示，可以分为三个步骤</p>
<ol>
<li><p>对图中每个顶点邻居顶点进行采样</p>
</li>
<li><p>根据聚合函数聚合邻居顶点蕴含的信息</p>
</li>
<li><p>得到图中各顶点的向量表示供下游任务使用</p>
</li>
</ol>
<p>但是GCN在NLP方面已经可以很好地实现。所以，这个方面的研究待定。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/02/GraphSAGE/" data-id="ckhnlm62l000pnng79euc2nhh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-GCN实现文本识别" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/30/GCN%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/" class="article-date">
  <time datetime="2020-07-30T02:09:16.000Z" itemprop="datePublished">2020-07-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/30/GCN%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/">GCN实现文本识别</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>自然语言处理</p>
<p>代码部分：<br>需要nltk库，然后下载语料库失败。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/30/GCN%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/" data-id="ckhnlm62f000fnng77zwe1v0x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-TMP" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/29/TMP/" class="article-date">
  <time datetime="2020-07-29T09:03:27.000Z" itemprop="datePublished">2020-07-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/29/TMP/">TMP</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近自己比较忙，而且，感觉自己的系统快不堪重负了。<br>而且，自己的需求又特别多。<br>不会做就不会死。<br>最近打算重装一下Ubuntu系统，之前在系统上累积的错误越来越多了，<br>然后，自己最近写关于神经网络的博客中，有很多公式，截图是真的累，所以，我有一个大胆的想法，不如换个框架吧，python的那个什么D框架想试一试。<br>然后，还有NLP自然语言处理的任务要做。<br>而且，国内的开源软件，没有找到对自己有用的，心情很难受。<br>我觉得自己还有很多事情可以做。<br>恩，最近我可能又要消失一段时间了。<br>很多的东西都只做了一部分，等我去填坑。<br>大家在做死的道路上，且做且珍惜。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/29/TMP/" data-id="ckhnlm62v001enng7bru7hewe" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-神经网络与深度学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-07-28T02:26:22.000Z" itemprop="datePublished">2020-07-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络与深度学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h1><p>(1)神经网络，一种美丽的，受生物启发的编程范例，使计算机能够从观测数据中学习<br>(2)深度学习，用于在神经网络中学习的强大功能集</p>
<p>神经网络和深度学习在为图片，语音识别和自然语言处理中的许多问题提供了解决方法，本文主要内容为神经网络和深度学习的入门书籍，以手写数字识别作为例子。<br>本文的代码会在Github上展示:???<br>首先，这是面向新手的入门课程，主要是讲解神经网络和深度学习的原理，所以，代码中并没有使用框架，而是要自己编写，为了易读性，代码并没有进行优化。<br>其环境为:???</p>
        
          <p class="article-more-link">
            <a href="/2020/07/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" data-id="ckhnlm65s009pnng7hody7dji" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-人工智能之图卷积神经网络GCN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B9%8B%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CGCN/" class="article-date">
  <time datetime="2020-07-27T01:01:56.000Z" itemprop="datePublished">2020-07-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B9%8B%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CGCN/">人工智能之图卷积神经网络GCN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B9%8B%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CGCN/" data-id="ckhnlm648004anng7f8fzepze" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Pytorch自然语言处理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/27/Pytorch%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" class="article-date">
  <time datetime="2020-07-27T00:33:51.000Z" itemprop="datePublished">2020-07-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/27/Pytorch%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">Pytorch自然语言处理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h1><p>资料来源：<br>PyTorch自然语言处理（Natural Language Processing with PyTorch）</p>
<p>主要内容:自然语言处理NLP和深度学习，公式部分没有详细的分析。<br>示例代码:原书中有Spacy模块，但我偏向于NLTK模块来实现</p>
<p>章节：</p>
<p>1.基础介绍<br>2.传统NLP快速回顾<br>3.神经网络基础组建<br>4.自然语言处理前馈网络Feed-Forward Networks<br>5.Embedding Words and Types<br>6.自然语言处理Sequence Modeling<br>7.自然语言处理Sequence Modeling<br>8.用于自然语言处理的高级Sequence Modeling<br>9.经典，前沿和后续步骤</p>
<h1 id="1-基础介绍"><a href="#1-基础介绍" class="headerlink" title="1.基础介绍"></a>1.基础介绍</h1><h2 id="1-1-有监督学习"><a href="#1-1-有监督学习" class="headerlink" title="1.1 有监督学习"></a>1.1 有监督学习</h2><p><img src="%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png"><br>如上图所示:<br>Data:输入数据集中包含下面两个<br>Observation:观察对象，用x来表示<br>Targets:与观察对象相对应的标签，用y来表示<br>Model:数学表达式或者是一个函数，它接收x观察值，并预测目标的标签值。<br>Parameters:参数化模型中的权重，使用符号为w<br>Prediction:预测，是模型在给定观察值的情况下，所猜测的目标的标签值，我们通常在其后面加上hat<br>Loss Function:<br>比较预测值和观察对象之间的差距，返回一个标量。损失值越低，模型对目标的预测效果越好，用L来表示损失函数。</p>
<p>结合上面的概念，用数学来表示:<br>一个数据集 D={X[i],y[i]}，i=1..n，有n个例子。<br>给定这个数据集，我们想要学习一个权值w参数化的函数(模型)f。换言之，我们对f的结构做一个假设，给定这个结构，权值w的学习值将充分表征模型。<br>对于一个给定的输入X,模型预测y_hat作为目标: y_hat = f(X;W)<br>在监督学习中，对于训练例子，我们知道观察的真正目标标签y。这个实例的损失将为 L(y, y_hat) 。然后，监督学习就变成了一个寻找最优参数/权值w的过程，从而使所有n个例子的累积损失最小化。</p>
<h2 id="1-2-随机-梯度下降法及反向传播"><a href="#1-2-随机-梯度下降法及反向传播" class="headerlink" title="1.2 (随机)梯度下降法及反向传播"></a>1.2 (随机)梯度下降法及反向传播</h2><p>利用(随机)梯度下降法进行训练，监督学习的目标是为给定的数据集选择参数值，使损失函数最小化。换句话说，这等价于在方程中求最小值。<br>我们知道梯度下降法是一种常见的求最小值的方法。回忆一下，在传统的梯度下降法中，我们对参数的一些初值进行猜测，并迭代更新这些参数，直到目标函数(损失函数)的计算值低于可接受阈值(即收敛准则)。<br>对于大型数据集，由于内存限制，在整个数据集上实现传统的梯度下降通常是不可能的，而且由于计算开销，速度非常慢。相反，通常采用一种近似的梯度下降称为随机梯度下降(SGD)。在随机情况下，数据点或数据点的子集是随机选择的，并计算该子集的梯度。当使用单个数据点时，这种方法称为纯SGD，当使用(多个)数据点的子集时，我们将其称为小型批处理SGD。<br>通常情况下，“纯”和“小型批处理”这两个词在根据上下文变得清晰时就会被删除。在实际应用中，很少使用纯SGD，因为它会由于有噪声的更新而导致非常慢的收敛。一般SGD算法有不同的变体，都是为了更快的收敛。在后面的章节中，我们将探讨这些变体中的一些，以及如何使用渐变来更新参数。这种迭代更新参数的过程称为反向传播。反向传播的每个步骤(又名epoch)由向前传递和向后传递组成。向前传递用参数的当前值计算输入并计算损失函数。反向传递使用损失梯度更新参数。</p>
<h2 id="1-3-观察对象和标签的编码"><a href="#1-3-观察对象和标签的编码" class="headerlink" title="1.3 观察对象和标签的编码"></a>1.3 观察对象和标签的编码</h2><p><img src="%E8%A7%82%E5%AF%9F%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%A0%87%E7%AD%BE%E7%9A%84%E7%BC%96%E7%A0%81.png"><br>如上图所示，我们需要用数字来表示观察值(文本)，以便与机器学习算法一起使用。<br>表示文本的一种简单的方法就是用数字向量来表示。我们使用其中一个简单的方法:</p>
<h3 id="1-3-1-One-Hot表示方式"><a href="#1-3-1-One-Hot表示方式" class="headerlink" title="1.3.1 One-Hot表示方式"></a>1.3.1 One-Hot表示方式</h3><p>one-hot表示从一个0向量开始，如果单词出现在句子或者文档中，则将向量中相应的条目设置为1,例如下面的两句话:<br>‘’’bash<br>Time flies like an arrow<br>Fruit flies like a banana<br>‘’’<br>对句子进行标记，忽略标点符号，然后将所有单词用小写字母来表示，就会得到一个大小为8的词汇表{time, fruit, flies, like, a, an, arrow, banana}。所以，我们可以用一个8维的one-hot向量来表示每一个单词，我们使用l[w]来表示单词w的one-hot表示。</p>
<p>对于短语、句子或者文档，其one-hot表示仅仅是它的组成词语的one-hot表示的逻辑或。</p>
<p>例如，短语’like a banana’的one-hot表示就是一个3×8的矩阵。<br>通常，会看到折叠形式的或二进制编码，其文本由和词汇表相同的向量表示，用0或1表示缺失或者存在。‘like a banana’的二进制编码是:[0,0,0,1,1,0,0,1]。备注，折叠one-hot是一个向量中有多个1的one-hot<br><img src="LikeABanana.png"></p>
<p>注意，可能对flies的两种意思弄混了，但是语言中充满了这种歧义，但是我们可以通过简单的假设来构建方案。使其学习特定意义的表示，但这个要之后再说。</p>
<p>本文章通常使用one-hot表示，但是在NLP中还有其他的表示方法。</p>
<h3 id="1-3-2-TF表示"><a href="#1-3-2-TF表示" class="headerlink" title="1.3.2 TF表示"></a>1.3.2 TF表示</h3><p>句子的TF表示仅仅是句子中词的one-hot总和。<br>使用前面的one-hot编码，“Fruit flies like time flies a fruit”这句话具有以下TF表示:[1,2,2,1,1,1,0,0]。<br>每一个数字是句子中出现相应单词的次数</p>
<h3 id="1-3-3-TF-IDF表示"><a href="#1-3-3-TF-IDF表示" class="headerlink" title="1.3.3 TF-IDF表示"></a>1.3.3 TF-IDF表示</h3><p>一个文件中可能会出现相同的词语很多次。<br>例如专利文件中，里面claim,system,method等单词会经常出现很多次。<br>TF表示方式对更频繁的词进行加权，但是，上面的词语不会增加我们对文章内容的理解，相反，如果类似tetrafluoroethylene这样的词语出现的频率比较低，但是很可能表明了专利文件的性质，所以希望给予它更加大的权重，反文档频率是一种启发式的算法，可以精确地做到这一点。</p>
<p>TF-IDF会降低常见词语的权重，而增加不常见词语的权重。<br>词频(TF)=某个词在文章中出现的次数 / 文章中的总词数<br>逆文档频率(IDF)=log(语料库的文档总数 / (包含该词的文档数+1))</p>
<p>TF-IDF=TF(w)*IDF(w)<br>假如所有文档中都有这个词语，那么数值为0,当一个词语很少出现，可能只出现在一个文档中，那么IDF就是最大值。</p>
<p>在深度学习中，很少看到使用像TF-IDF这样的启发式表示对输入进行编码，因为目标是学习一种表示。<br>通常，我们从一个使用整数索引的one-hot编码和一个特殊的“embedding lookup”层开始构建神经网络的输入。</p>
<h3 id="1-3-4-目标编码"><a href="#1-3-4-目标编码" class="headerlink" title="1.3.4 目标编码"></a>1.3.4 目标编码</h3><p>目标变量的性质取决与所要解决的NLP任务。例如，在机器翻译、摘要和回答问题的情况下，目标是文本，并且使用前面描述的one-hot编码方法进行编码。</p>
<p>许多NLP任务实际上是使用分类标签，其中模型必须预测一组固定标签中的一个。对于这种编码的常见方法是对每一个标签使用唯一的索引。当输出标签的数量太大时，这种简单的表示可能会出现问题。这方面的一个例子是语言建模问题，在这个问题中，任务是预测下一个单词，给定过去看到的单词。标签空间是一种语言的全部词汇，它可以很容易地增长到几十万，包括特殊字符、名称等等。</p>
<p>一些NLP问题涉及从给定文本中预测一个数值。例如，给定一篇英语文章，我们可能需要分配一个数字评分或可读性评分。给定一个餐馆评论片段，我们可能需要预测直到小数点后第一位的星级。给定用户的推文，我们可能需要预测用户的年龄群。有几种方法可以对数字目标进行编码，但是将目标简单地绑定到分类“容器”中(例如，“0-18”、“19-25”、“25-30”等等)，并将其视为有序分类问题是一种合理的方法。 这一部分，超过了我这个文章的范围，在这种情况下，目标编码会显著影响性能。</p>
<h2 id="1-4-计算图"><a href="#1-4-计算图" class="headerlink" title="1.4 计算图"></a>1.4 计算图</h2><p>将上面的模型对输入进行转换，从而获得预测。损失函数提供反馈信号来调整模型的参数。利用计算图数据结构可以方便地实现该数据流。从技术上讲，计算图是对数学表达式建模的抽象。在深度学习中，计算图的实现(Theano、TensorFlow和PyTorch)进行了额外的记录，以实现在监督学习中训练期间获取参数梯度所需要的自动微分。我们将在PyTorch基础知识中探讨。推理或者预测就是见打的表达式求值，计算图上的正向流。<br>考虑表达式:y=wx+b<br>将其写成两个子表达式:z=wx和y=z+b,我们使用一个有向无环图DAG来表示原始的表达式。<br><img src="%E8%AE%A1%E7%AE%97%E5%9B%BE.png"><br>之后，我们将看到如何用PyTorch以直观的方式创建计算图形，以及它如何让我们计算梯度，而无需考虑任何记录(bookkeeping)</p>
<p>PyTorch基础：<br>PyTorch实现了一种“tape-based automatic differentiation”方法，允许我们动态定义和执行计算图形。这对于调试和用最少的努力构建复杂的模型非常有帮助。</p>
<p>动态 VS 静态计算图 像Theano、Caffe和TensorFlow这样的静态框架需要首先声明、编译和执行计算图。虽然这会导致非常高效的实现(在生产和移动设置中非常有用)，但在研究和开发过程中可能会变得非常麻烦。像Chainer、DyNet和PyTorch这样的现代框架实现了动态计算图，从而支持更灵活的命令式开发风格，而不需要在每次执行之前编译模型。动态计算图在建模NLP任务时特别有用，每个输入可能导致不同的图结构。</p>
<p>PyTorch是一个优化的张量操作库，它提供了一系列用于深度学习的包。这个库的核心是张量，它是一个包含一些多维数据的数学对象。0阶张量就是一个数字，或者标量。一阶张量(一阶张量)是一个数字数组，或者说是一个向量。类似地，二阶张量是一个向量数组，或者说是一个矩阵。因此，张量可以推广为标量的n维数组，</p>
<h1 id="2-传统NLP快速回顾"><a href="#2-传统NLP快速回顾" class="headerlink" title="2.传统NLP快速回顾"></a>2.传统NLP快速回顾</h1><p>自然语言处理(NLP)和计算语言学(CL)是人类语言计算研究的两个领域。NLP旨在开发解决涉及语言的实际问题的方法，如信息提取、自动语音识别、机器翻译、情绪分析、问答和总结。另一方面，CL使用计算方法来理解人类语言的特性。我们如何理解语言?我们如何产生语言?我们如何学习语言?语言之间有什么关系?</p>
<p>在文献中，我们经常看到方法和研究人员的交叉，从CL到NLP，反之亦然。来自语言学习的课程内容可以用来告知NLP中的先验，统计和机器学习的方法可以用来回答CL想要回答的问题。事实上，这些问题中的一些已经扩展到它们自己的学科，如音位学、形态学、句法学、语义学和语用学。</p>
<p>在本书中，我们只关注NLP，但是我们经常根据需要从CL中借鉴思想。在我们将自己完全归属于NLP的神经网络方法之前，有必要回顾一下一些传统的NLP概念和方法。这就是本章的目标。</p>
<h2 id="2-1-语料库Corpora、令牌Tokens、Types"><a href="#2-1-语料库Corpora、令牌Tokens、Types" class="headerlink" title="2.1 语料库Corpora、令牌Tokens、Types"></a>2.1 语料库Corpora、令牌Tokens、Types</h2><p>所有的NLP方法，无论是经典的还是现代的，都以文本数据集开始，也称为语料库(复数:corpora)。语料库通常有原始文本(ASCII或UTF-8格式)和与文本相关的任何元数据。原始文本是字符(字节)序列，但是大多数时候将字符分组成连续的称为令牌(Tokens)的连续单元是有用的。在英语中，令牌(Tokens)对应由空格字符或标点分隔的单词和数字序列。</p>
<p>元数据(Metadata)可以是与文本相关联的任何辅助信息，例如标识符，标签和时间戳。 在机器学习术语中，文本及其元数据称为实例(instance)或数据点。 下面是语料库的一组实例，也称为数据集。 鉴于本书重点关注机器学习，我们可以自由地交换术语语料库和数据集。<br><img src="%E8%AF%AD%E6%96%99%E5%BA%93.png"></p>
<p>将文本分为令牌Tokens的过程称为令牌化tokenization。</p>
<p>首先，以下面的推特作为例子:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Snow White and Seven Degress <span class="comment">#MakeAMovieCold @midnight</span></span><br></pre></td></tr></table></figure>
<p>上面的#MakeMovieCold标签应该是一个令牌还是4个令牌，许多论文没有太多的关注，许多令牌化决策往往是任意的，但是，这些决策在实践中对准确性的影响比公认的大很多，通常被认为是预处理的繁琐工作，大多数开源NLP包为令牌化提供了合理的支持，举了NLTK和Spacy的例子，这是两个用于文本处理的常用包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Input[1]:</span><br><span class="line">from nltk.tokenize import TweetTokenizer</span><br><span class="line">tweet=u<span class="string">&quot;Snow White and the Seven Degrees</span></span><br><span class="line"><span class="string">    #MakeAMovieCold@midnight:-)&quot;</span></span><br><span class="line">tokenizer = TweetTokenizer()</span><br><span class="line"><span class="built_in">print</span>(tokenizer.tokenize(tweet.lower()))</span><br><span class="line">Output[1]:</span><br><span class="line">[<span class="string">&#x27;snow&#x27;</span>, <span class="string">&#x27;white&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;seven&#x27;</span>, <span class="string">&#x27;degrees&#x27;</span>, <span class="string">&#x27;#makeamoviecold&#x27;</span>, <span class="string">&#x27;@midnight&#x27;</span>, <span class="string">&#x27;:-)&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>类型是语料库中的唯一的令牌，语料库中所有类型的集合就是它的词汇表或者词典。词可以分为内容词和停止词。像冠词和介词这样的限定词主要是为了达到语法的目的，就像填充物来支撑着内容词一样。</p>
<h2 id="2-2-Unigrams-Bigrams-Trigrams-…-Ngrams"><a href="#2-2-Unigrams-Bigrams-Trigrams-…-Ngrams" class="headerlink" title="2.2 Unigrams,Bigrams,Trigrams,…,Ngrams"></a>2.2 Unigrams,Bigrams,Trigrams,…,Ngrams</h2><p>ngram是文本中出现的固定长度n的连续令牌序列。bigram有两个令牌，unigram只有一个令牌。从文本生成ngram非常简单，但是nltk提供了简单方法。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def n_grams(text, n):</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">    takes tokens or text, returns a list of n grams</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">return</span> [text[i:i+n] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(text)-n+1)]</span><br><span class="line"></span><br><span class="line">cleaned = [<span class="string">&#x27;mary&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&quot;n&#x27;t&quot;</span>, <span class="string">&#x27;slap&#x27;</span>, green<span class="string">&#x27;, &#x27;</span>witch<span class="string">&#x27;, &#x27;</span>.<span class="string">&#x27;]</span></span><br><span class="line"><span class="string">print(n_grams(cleaned, 3))</span></span><br></pre></td></tr></table></figure>
<p>对于子词(subword)信息本身携带有用信息的某些情况，可能需要生成字符ngram。例如，“methanol”中的后缀“-ol”表示它是一种醇;如果您的任务涉及到对有机化合物名称进行分类，那么您可以看到ngram捕获的子单词(subword)信息是如何有用的。在这种情况下，您可以重用相同的代码，除了将每个字符ngram视为令牌。(这里的subword应该是值类似前缀后缀这种完整单词中的一部分)</p>
<h2 id="2-3-Categorizing-Sentences-and-Documents-对文档或句子进行归类"><a href="#2-3-Categorizing-Sentences-and-Documents-对文档或句子进行归类" class="headerlink" title="2.3 Categorizing Sentences and Documents 对文档或句子进行归类"></a>2.3 Categorizing Sentences and Documents 对文档或句子进行归类</h2><p>对文档或句子进行归类可能是NLP最早的应用之一，我们在之前的TF和TF-IDF表示中，对于较长的文本块，例如文档或者句子进行分类非常有用。主题标签的分配、评论情绪的检测、垃圾邮件的过滤，语言识别和邮件分类等问题可以定义为受监督的文档分类问题。<br>还有一种半监督的方法，只用了一个小的部分数据，但是这个方法暂时不考虑。</p>
<h2 id="2-4-对字进行分类-POS-标签"><a href="#2-4-对字进行分类-POS-标签" class="headerlink" title="2.4 对字进行分类 POS 标签"></a>2.4 对字进行分类 POS 标签</h2><p>我们可以将标记的概念从文档扩展到单词或者标记。分类词的一个常见的方法是词性标注。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line"></span><br><span class="line">document = <span class="string">&#x27;Whether you\&#x27;</span>re new to programming or an experienced developer, it\<span class="string">&#x27;s easy to learn and use Python.&#x27;</span></span><br><span class="line">sentences = nltk.sent_tokenize(document)</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> sentences:</span><br><span class="line">    <span class="built_in">print</span>(nltk.pos_tag(nltk.word_tokenize(sent)))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;Whether&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>), (<span class="string">&#x27;you&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>), (<span class="string">&quot;&#x27;re&quot;</span>, <span class="string">&#x27;VBP&#x27;</span>), (<span class="string">&#x27;new&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;TO&#x27;</span>), (<span class="string">&#x27;programming&#x27;</span>, <span class="string">&#x27;VBG&#x27;</span>), (<span class="string">&#x27;or&#x27;</span>, <span class="string">&#x27;CC&#x27;</span>), (<span class="string">&#x27;an&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), (<span class="string">&#x27;experienced&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;developer&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>), (<span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;,&#x27;</span>), (<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>), (<span class="string">&quot;&#x27;s&quot;</span>, <span class="string">&#x27;VBZ&#x27;</span>), (<span class="string">&#x27;easy&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;TO&#x27;</span>), (<span class="string">&#x27;learn&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>), (<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;CC&#x27;</span>), (<span class="string">&#x27;use&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>), (<span class="string">&#x27;Python&#x27;</span>, <span class="string">&#x27;NNP&#x27;</span>), (<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;.&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>词性的表达方式:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CC  并列连词          NNS 名词复数        UH 感叹词</span><br><span class="line">CD  基数词              NNP 专有名词        VB 动词原型</span><br><span class="line">DT  限定符            NNP 专有名词复数    VBD 动词过去式</span><br><span class="line">EX  存在词            PDT 前置限定词      VBG 动名词或现在分词</span><br><span class="line">FW  外来词            POS 所有格结尾      VBN 动词过去分词</span><br><span class="line">IN  介词或从属连词     PRP 人称代词        VBP 非第三人称单数的现在时</span><br><span class="line">JJ  形容词            PRP$ 所有格代词     VBZ 第三人称单数的现在时</span><br><span class="line">JJR 比较级的形容词     RB  副词            WDT 以wh开头的限定词</span><br><span class="line">JJS 最高级的形容词     RBR 副词比较级      WP 以wh开头的代词</span><br><span class="line">LS  列表项标记         RBS 副词最高级      WP$ 以wh开头的所有格代词</span><br><span class="line">MD  情态动词           RP  小品词          WRB 以wh开头的副词</span><br><span class="line">NN  名词单数           SYM 符号            TO  to</span><br></pre></td></tr></table></figure>

<h2 id="2-5-分块Chunking和浅解析Shallow-parsing"><a href="#2-5-分块Chunking和浅解析Shallow-parsing" class="headerlink" title="2.5 分块Chunking和浅解析Shallow parsing"></a>2.5 分块Chunking和浅解析Shallow parsing</h2><p>浅解析的目的是，推导出由名词、动词、形容词等语法原子组成的高阶的单位，如果没有训练浅解析模型的数据，可以在词性标记上编写正则表达式来近似浅解析。幸运的是，对于英语和最广泛使用的语言来说，这样的数据和预先训练的模型是存在的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-6-句子树"><a href="#2-6-句子树" class="headerlink" title="2.6 句子树"></a>2.6 句子树</h2><p>浅层解析识别短语单位，而识别它们之间关系的任务称为解析(parsing)。例如，用图来表示句子。<br><img src="%E5%8F%A5%E5%AD%90%E6%A0%91.png"></p>
<p>解析树(Parse tree)表示句子中不同的语法单元在层次上是如何相关的。另一种可能更有用的显示关系的方法是使用依赖项解析(dependency parsing)，如下图表示:<br><img src="%E4%BE%9D%E8%B5%96%E8%A7%A3%E6%9E%90.png"></p>
<h2 id="2-7-单词含义"><a href="#2-7-单词含义" class="headerlink" title="2.7 单词含义"></a>2.7 单词含义</h2><p>单词有意义，而且通常不止一个。一个词的不同含义称为它的意义(senses)。WordNet是一个长期运行的词汇资源项目，它来自普林斯顿大学，旨在对所有英语单词(嗯，大部分)的含义以及其他词汇关系进行分类。<br><img src="%E8%AF%8D%E6%B1%87%E8%B5%84%E6%BA%90%E9%A1%B9%E7%9B%AE.png"></p>
<p>上面的WordNet项目已经花费了数十年，即使在现在的方法，例如神经网络和深度学习方法的背景下使用现有的语言资源。<br>词的意义也可以从上下文中归纳出来。从文本中自动发现词义实际上是半监督学习在自然语言处理中的第一个应用。尽管这部分没有详细学习，但资源:Jurasky and Martin(2014)，第17章，Manning and Schutze(1999)，第7章</p>
<p>参考文献:<br>Manning, Christopher D., and Hinrich Schütze. (1999). Foundations of statistical natural language processing. MIT press.</p>
<p>Bird, Steven, Ewan Klein, and Edward Loper. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. O’Reilly Media.</p>
<p>Smith, Noah A. (2011). “Linguistic structure prediction.” Synthesis lectures on human language technologies.</p>
<p>Jurafsky, Dan, and James H. Martin. (2014). Speech and language processing. Vol. 3. London: Pearson.</p>
<p>Russell, Stuart J., and Peter Norvig. (2016). Artificial intelligence: a modern approach. Malaysia: Pearson Education Limited.</p>
<p>Zheng, Alice, and Casari, Amanda. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O’Reilly Media, Inc.</p>
<h1 id="3-神经网络基础组建"><a href="#3-神经网络基础组建" class="headerlink" title="3.神经网络基础组建"></a>3.神经网络基础组建</h1><p>这部分的内容可以直接参考我的博客上神经网络与深度学习的基础知识(Emir-Liu.github.io)。<br><img src="%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png"></p>
<p>在这个例子中，我们使用Yelp数据集，它将评论与它们的情感标签(正面或负面)配对。此外，我们还描述了一些数据集操作步骤，这些步骤用于清理数据集并将其划分为训练、验证和测试集。</p>
<p>在理解数据集之后，您将看到定义三个辅助类的模式，这三个类在本书中反复出现，用于将文本数据转换为向量化的形式:词汇表(the Vocabulary)、向量化器(Vectorizer)和PyTorch的DataLoader。<br>词汇表协调我们在“观察和目标编码”中讨论的整数到令牌(token)映射。我们使用一个词汇表将文本标记(text tokens)映射到整数，并将类标签映射到整数。<br>接下来，矢量化器(vectorizer)封装词汇表，并负责接收字符串数据，如审阅文本，并将其转换为将在训练例程中使用的数字向量。<br>我们使用最后一个辅助类，PyTorch的DataLoader，将单个向量化数据点分组并整理成minibatches。</p>
<h2 id="3-1-The-Yelp-Review-Dataset"><a href="#3-1-The-Yelp-Review-Dataset" class="headerlink" title="3.1 The Yelp Review Dataset"></a>3.1 The Yelp Review Dataset</h2><p>2015年，Yelp举办了一场竞赛，要求参与者根据点评预测一家餐厅的评级。<br>同年，Zhang, Zhao，和Lecun(2015)将1星和2星评级转换为“消极”情绪类，将3星和4星评级转换为“积极”情绪类，从而简化了数据集。该数据集分为56万个训练样本和3.8万个测试样本。在这个数据集部分的其余部分中，我们将描述最小化清理数据并导出最终数据集的过程。然后，我们概述了利用PyTorch的数据集类的实现。</p>
<p>在这个例子中，我们使用了简化的Yelp数据集，但是有两个细微的区别。第一个区别是我们使用数据集的“轻量级”版本，它是通过选择10%的训练样本作为完整数据集而派生出来的。这有两个结果:首先，使用一个小数据集可以使训练测试循环快速，因此我们可以快速地进行实验。其次，它生成的模型精度低于使用所有数据。这种低精度通常不是主要问题，因为您可以使用从较小数据集子集中获得的知识对整个数据集进行重新训练。在训练深度学习模型时，这是一个非常有用的技巧，因为在许多情况下，训练数据的数量是巨大的。</p>
<p>从这个较小的子集中，我们将数据集分成三个分区:一个用于训练，一个用于验证，一个用于测试。虽然原始数据集只有两个部分，但是有一个验证集是很重要的。在机器学习中，您经常在数据集的训练部分上训练模型，并且需要一个held-out部分来评估模型的性能。如果模型决策基于held-out部分，那么模型现在不可避免地偏向于更好地执行held-out部分。因为度量增量进度是至关重要的，所以这个问题的解决方案是使用第三个部分，它尽可能少地用于评估。</p>
<p>综上所述，您应该使用数据集的训练部分来派生模型参数，使用数据集的验证部分在超参数之间进行选择(进行建模决策)，使用数据集的测试分区进行最终评估和报告。</p>
<h1 id="4-自然语言处理前馈网络Feed-Forward-Networks"><a href="#4-自然语言处理前馈网络Feed-Forward-Networks" class="headerlink" title="4.自然语言处理前馈网络Feed-Forward Networks"></a>4.自然语言处理前馈网络Feed-Forward Networks</h1><h1 id="5-Embedding-Words-and-Types"><a href="#5-Embedding-Words-and-Types" class="headerlink" title="5.Embedding Words and Types"></a>5.Embedding Words and Types</h1><h1 id="6-自然语言处理Sequence-Modeling"><a href="#6-自然语言处理Sequence-Modeling" class="headerlink" title="6.自然语言处理Sequence Modeling"></a>6.自然语言处理Sequence Modeling</h1><h1 id="7-自然语言处理Sequence-Modeling"><a href="#7-自然语言处理Sequence-Modeling" class="headerlink" title="7.自然语言处理Sequence Modeling"></a>7.自然语言处理Sequence Modeling</h1><h1 id="8-用于自然语言处理的高级Sequence-Modeling"><a href="#8-用于自然语言处理的高级Sequence-Modeling" class="headerlink" title="8.用于自然语言处理的高级Sequence Modeling"></a>8.用于自然语言处理的高级Sequence Modeling</h1><h1 id="9-经典，前沿和后续步骤"><a href="#9-经典，前沿和后续步骤" class="headerlink" title="9.经典，前沿和后续步骤"></a>9.经典，前沿和后续步骤</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/27/Pytorch%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" data-id="ckhnlm65p009hnng78qy647s3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/FAQ/" rel="tag">FAQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/adaboost/" rel="tag">adaboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/aircrack-ng/" rel="tag">aircrack-ng</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/anaconda/" rel="tag">anaconda</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/apache/" rel="tag">apache</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/apt/" rel="tag">apt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/beautifulsoup/" rel="tag">beautifulsoup</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bencode/" rel="tag">bencode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bert/" rel="tag">bert</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bitTorrent/" rel="tag">bitTorrent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bitcoin/" rel="tag">bitcoin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c#</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c-c/" rel="tag">c/c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/codeblocks/" rel="tag">codeblocks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/expect/" rel="tag">expect</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ftp/" rel="tag">ftp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gnn/" rel="tag">gnn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gpt/" rel="tag">gpt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/grep/" rel="tag">grep</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gtk/" rel="tag">gtk+</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/html-css/" rel="tag">html/css</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k%E7%BA%BF%E5%9B%BE/" rel="tag">k线图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/libreoffice/" rel="tag">libreoffice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/makefile/" rel="tag">makefile</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/" rel="tag">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/metasploit/" rel="tag">metasploit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mono/" rel="tag">mono</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/opencv/" rel="tag">opencv</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/p2p/" rel="tag">p2p</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/" rel="tag">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/php/" rel="tag">php</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/re/" rel="tag">re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/schedule/" rel="tag">schedule</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scp/" rel="tag">scp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsocks/" rel="tag">shadowsocks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/" rel="tag">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/" rel="tag">ssh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stm32/" rel="tag">stm32</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tkinker/" rel="tag">tkinker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/transformer/" rel="tag">transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu/" rel="tag">ubuntu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/uc-os/" rel="tag">uc/os</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vim/" rel="tag">vim</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web/" rel="tag">web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B9%A6%E5%8D%95/" rel="tag">书单</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/" rel="tag">交叉熵</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" rel="tag">信息论</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%B7%E7%A7%AF/" rel="tag">卷积</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%9F%E5%A7%8BGNN/" rel="tag">原始GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%97%E6%95%99/" rel="tag">宗教</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%B1%E8%A7%86/" rel="tag">影视</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC/" rel="tag">执行脚本</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E5%A4%84%E7%90%86/" rel="tag">批处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%8B%AC/" rel="tag">数独</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A1%8C%E6%B8%B8/" rel="tag">桌游</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F/" rel="tag">游戏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%90%86%E8%AE%BA/" rel="tag">理论</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" rel="tag">知识图谱</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" rel="tag">科学上网</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E7%A8%8B/" rel="tag">线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E7%AB%99/" rel="tag">网站</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E7%BB%9C/" rel="tag">网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E8%AE%BF%E9%97%AE%E5%8A%A9%E6%89%8B/" rel="tag">谷歌访问助手</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%9B%E7%A8%8B/" rel="tag">进程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E6%9C%BA%E6%95%B0/" rel="tag">随机数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/FAQ/" style="font-size: 10px;">FAQ</a> <a href="/tags/GCN/" style="font-size: 10px;">GCN</a> <a href="/tags/adaboost/" style="font-size: 10px;">adaboost</a> <a href="/tags/aircrack-ng/" style="font-size: 10px;">aircrack-ng</a> <a href="/tags/anaconda/" style="font-size: 10px;">anaconda</a> <a href="/tags/apache/" style="font-size: 10px;">apache</a> <a href="/tags/apt/" style="font-size: 10px;">apt</a> <a href="/tags/beautifulsoup/" style="font-size: 10px;">beautifulsoup</a> <a href="/tags/bencode/" style="font-size: 10px;">bencode</a> <a href="/tags/bert/" style="font-size: 10px;">bert</a> <a href="/tags/bitTorrent/" style="font-size: 11.43px;">bitTorrent</a> <a href="/tags/bitcoin/" style="font-size: 10px;">bitcoin</a> <a href="/tags/c/" style="font-size: 10px;">c#</a> <a href="/tags/c-c/" style="font-size: 15.71px;">c/c++</a> <a href="/tags/codeblocks/" style="font-size: 10px;">codeblocks</a> <a href="/tags/expect/" style="font-size: 10px;">expect</a> <a href="/tags/ftp/" style="font-size: 10px;">ftp</a> <a href="/tags/git/" style="font-size: 12.86px;">git</a> <a href="/tags/gnn/" style="font-size: 10px;">gnn</a> <a href="/tags/gpt/" style="font-size: 11.43px;">gpt</a> <a href="/tags/grep/" style="font-size: 10px;">grep</a> <a href="/tags/gtk/" style="font-size: 10px;">gtk+</a> <a href="/tags/hexo/" style="font-size: 11.43px;">hexo</a> <a href="/tags/html-css/" style="font-size: 10px;">html/css</a> <a href="/tags/k%E7%BA%BF%E5%9B%BE/" style="font-size: 10px;">k线图</a> <a href="/tags/libreoffice/" style="font-size: 10px;">libreoffice</a> <a href="/tags/makefile/" style="font-size: 10px;">makefile</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/metasploit/" style="font-size: 10px;">metasploit</a> <a href="/tags/mono/" style="font-size: 10px;">mono</a> <a href="/tags/mysql/" style="font-size: 14.29px;">mysql</a> <a href="/tags/nlp/" style="font-size: 12.86px;">nlp</a> <a href="/tags/opencv/" style="font-size: 17.14px;">opencv</a> <a href="/tags/p2p/" style="font-size: 10px;">p2p</a> <a href="/tags/pandas/" style="font-size: 10px;">pandas</a> <a href="/tags/php/" style="font-size: 12.86px;">php</a> <a href="/tags/python/" style="font-size: 18.57px;">python</a> <a href="/tags/pytorch/" style="font-size: 10px;">pytorch</a> <a href="/tags/re/" style="font-size: 10px;">re</a> <a href="/tags/schedule/" style="font-size: 10px;">schedule</a> <a href="/tags/scp/" style="font-size: 10px;">scp</a> <a href="/tags/shadowsocks/" style="font-size: 10px;">shadowsocks</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/socket/" style="font-size: 11.43px;">socket</a> <a href="/tags/ssh/" style="font-size: 11.43px;">ssh</a> <a href="/tags/stm32/" style="font-size: 17.14px;">stm32</a> <a href="/tags/tkinker/" style="font-size: 10px;">tkinker</a> <a href="/tags/transformer/" style="font-size: 10px;">transformer</a> <a href="/tags/ubuntu/" style="font-size: 20px;">ubuntu</a> <a href="/tags/uc-os/" style="font-size: 10px;">uc/os</a> <a href="/tags/vim/" style="font-size: 11.43px;">vim</a> <a href="/tags/web/" style="font-size: 11.43px;">web</a> <a href="/tags/%E4%B9%A6%E5%8D%95/" style="font-size: 10px;">书单</a> <a href="/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/" style="font-size: 10px;">交叉熵</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" style="font-size: 10px;">信息论</a> <a href="/tags/%E5%8D%B7%E7%A7%AF/" style="font-size: 10px;">卷积</a> <a href="/tags/%E5%8E%9F%E5%A7%8BGNN/" style="font-size: 10px;">原始GNN</a> <a href="/tags/%E5%AE%97%E6%95%99/" style="font-size: 10px;">宗教</a> <a href="/tags/%E5%BD%B1%E8%A7%86/" style="font-size: 10px;">影视</a> <a href="/tags/%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC/" style="font-size: 10px;">执行脚本</a> <a href="/tags/%E6%89%B9%E5%A4%84%E7%90%86/" style="font-size: 10px;">批处理</a> <a href="/tags/%E6%95%B0%E7%8B%AC/" style="font-size: 10px;">数独</a> <a href="/tags/%E6%A1%8C%E6%B8%B8/" style="font-size: 10px;">桌游</a> <a href="/tags/%E6%B8%B8%E6%88%8F/" style="font-size: 10px;">游戏</a> <a href="/tags/%E7%90%86%E8%AE%BA/" style="font-size: 10px;">理论</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" style="font-size: 10px;">知识图谱</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a> <a href="/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" style="font-size: 10px;">科学上网</a> <a href="/tags/%E7%BA%BF%E7%A8%8B/" style="font-size: 10px;">线程</a> <a href="/tags/%E7%BD%91%E7%AB%99/" style="font-size: 10px;">网站</a> <a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">网络</a> <a href="/tags/%E8%B0%B7%E6%AD%8C%E8%AE%BF%E9%97%AE%E5%8A%A9%E6%89%8B/" style="font-size: 10px;">谷歌访问助手</a> <a href="/tags/%E8%BF%9B%E7%A8%8B/" style="font-size: 10px;">进程</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E6%95%B0/" style="font-size: 10px;">随机数</a> <a href="/tags/%E9%9A%8F%E7%AC%94/" style="font-size: 10px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/11/24/Python%E6%A8%A1%E5%9D%97%E6%89%93%E5%8C%85%E5%8F%8A%E5%8F%91%E5%B8%83/">Python模块打包及发布</a>
          </li>
        
          <li>
            <a href="/2020/11/23/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%9E%8D%E5%90%88/">知识图谱融合</a>
          </li>
        
          <li>
            <a href="/2020/11/23/Schedule/">Schedule</a>
          </li>
        
          <li>
            <a href="/2020/11/18/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2020/10/23/AdaBoost%E6%A8%A1%E5%9E%8B/">AdaBoost模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>