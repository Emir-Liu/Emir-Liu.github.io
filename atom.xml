<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-12-24T04:18:21.834Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GRU说明</title>
    <link href="http://example.com/2020/12/24/GRU%E8%AF%B4%E6%98%8E/"/>
    <id>http://example.com/2020/12/24/GRU%E8%AF%B4%E6%98%8E/</id>
    <published>2020-12-24T04:18:21.000Z</published>
    <updated>2020-12-24T04:18:21.834Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>配置文件参考说明</title>
    <link href="http://example.com/2020/12/22/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%8F%82%E8%80%83%E8%AF%B4%E6%98%8E/"/>
    <id>http://example.com/2020/12/22/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%8F%82%E8%80%83%E8%AF%B4%E6%98%8E/</id>
    <published>2020-12-22T06:56:32.000Z</published>
    <updated>2020-12-22T07:29:41.360Z</updated>
    
    <content type="html"><![CDATA[<p>在很多情况下，我们需要将一些参数保存在程序的外面，也就是作为配置文件进行保存，或者，提前写好配置文件，从中读取必须的参数，方便实现。</p><p>但是有很多的可以作为配置文件:<br>YAML<br>JSON<br>好像没有注释<br>ini<br>XML</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在很多情况下，我们需要将一些参数保存在程序的外面，也就是作为配置文件进行保存，或者，提前写好配置文件，从中读取必须的参数，方便实现。&lt;/p&gt;
&lt;p&gt;但是有很多的可以作为配置文件:&lt;br&gt;YAML&lt;br&gt;JSON&lt;br&gt;好像没有注释&lt;br&gt;ini&lt;br&gt;XML&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Python中利用seqeval模块进行序列标注算法的模型评估</title>
    <link href="http://example.com/2020/12/21/Python%E4%B8%AD%E5%88%A9%E7%94%A8seqeval%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E7%AE%97%E6%B3%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"/>
    <id>http://example.com/2020/12/21/Python%E4%B8%AD%E5%88%A9%E7%94%A8seqeval%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E7%AE%97%E6%B3%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/</id>
    <published>2020-12-21T07:40:01.000Z</published>
    <updated>2020-12-21T10:43:18.389Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h1><p>在NLP任务中，我们经常需要使用序列标注算法，为此，我们需要评估该模型在序列标注任务中的效果，这里使用了seqeval模块。</p><p>一般而言，序列标注算法的格式有BIO、IOBES、BMES等。</p><p>模型的评价指标有，一般只会注意英文，中文容易弄混:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">真实值\预测值   |Positive       |Negative       |</span><br><span class="line">Positive        |True Positive  |False Negative |</span><br><span class="line">Negative        |False Positive |True Negative  |</span><br></pre></td></tr></table></figure><p>Precision = TP/(TP+FP)<br>预测为正的样本中有多少预测对了</p><p>Recall = TP/(TP+FN)<br>真实为正的样本中有多少预测对了</p><p>Accuracy = (TP+TN)/(TP+TN+FP+FN)</p><p>F1 Score = 1/2(1/recall + 1/precision)<br>= 2Recall*Precision/(Recall+Precision)</p><h1 id="1-样例"><a href="#1-样例" class="headerlink" title="1.样例"></a>1.样例</h1><p>参考官网资料</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from seqeval.metrics import accuracy_score,classification_report,f1_score</span><br><span class="line"></span><br><span class="line">y_true = [[<span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>, <span class="string">&#x27;O&#x27;</span>], [<span class="string">&#x27;B- PER&#x27;</span>, <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">y_pred = [[<span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>, <span class="string">&#x27;O&#x27;</span>],[<span class="string">&#x27;B-PER&#x27;</span>, <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(f1_score(y_true, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_true, y_pred))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-概述&quot;&gt;&lt;a href=&quot;#0-概述&quot; class=&quot;headerlink&quot; title=&quot;0. 概述&quot;&gt;&lt;/a&gt;0. 概述&lt;/h1&gt;&lt;p&gt;在NLP任务中，我们经常需要使用序列标注算法，为此，我们需要评估该模型在序列标注任务中的效果，这里使用了seqeval模</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="seqeval" scheme="http://example.com/tags/seqeval/"/>
    
  </entry>
  
  <entry>
    <title>Python之版本相争</title>
    <link href="http://example.com/2020/12/17/Python%E4%B9%8B%E7%89%88%E6%9C%AC%E7%9B%B8%E4%BA%89/"/>
    <id>http://example.com/2020/12/17/Python%E4%B9%8B%E7%89%88%E6%9C%AC%E7%9B%B8%E4%BA%89/</id>
    <published>2020-12-17T07:53:29.000Z</published>
    <updated>2020-12-17T07:57:25.883Z</updated>
    
    <content type="html"><![CDATA[<p>在Ubuntu 20 版本的时候，不支持直接下载python2的pip ,所以，有下面的解决方法</p><ol><li><p>Start by enabling the universe repository:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository universe</span><br></pre></td></tr></table></figure></li><li><p>Update the packages index and install Python 2:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install python2</span><br></pre></td></tr></table></figure></li><li><p>Use curl to download the get-pip.py script:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://bootstrap.pypa.io/get-pip.py --output get-pip.py</span><br></pre></td></tr></table></figure></li><li><p>Once the repository is enabled, run the script as sudo user with python2 to install pip :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo python2 get-pip.py</span><br></pre></td></tr></table></figure></li><li><p>Verify the installation</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip2 --version</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在Ubuntu 20 版本的时候，不支持直接下载python2的pip ,所以，有下面的解决方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start by enabling the universe repository:&lt;/p&gt;
&lt;figure class=&quot;highlight </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LSTM分析</title>
    <link href="http://example.com/2020/12/15/LSTM%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2020/12/15/LSTM%E5%88%86%E6%9E%90/</id>
    <published>2020-12-15T07:46:44.000Z</published>
    <updated>2020-12-16T08:51:31.093Z</updated>
    
    <content type="html"><![CDATA[<p>关于LSTM模型:<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/LSTM%E6%A8%A1%E5%9E%8B.png"><br>参考:<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>细胞状态是LSTM的核心，如下图所示。<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81.png"><br>细胞状态就像是传送带，在整个链条中一直延伸，只有一些小的线性作用，有利于信息不加改变地流动。</p><p>LSTM模型通过门结构调节，具有向细胞状态删除或者增加信息的能力。</p><p>有三个门结构，来保护控制细胞状态。</p><p>forget gate layer<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/forget_layer.png"><br>我们首先需要确定之前的信息需要忘记多少<br>取决于h_t-1和x_t,输出0到1之间的数字。<br>1表示完全保留之前的信息C_t-1,0表示完全忘记之前的信息</p><p>input gate layer<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/input_layer.png"><br>然后，我们需要决定当前细胞状态需要加入多少新的信息。<br>这里有两个部分，第一个是sigmoid函数，叫做输入门层，决定了我们更新的系数i。然后一个tanh函数建立了一个候选的数值Ct，在下一步中，我们将会将这两个合并去更新状态。</p><p>更新旧的状态Ct-1<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/update.png"><br>这样，看上面的公式就很简单，首先是对之前的状态乘以一个系数，忘记一些信息，然后增加当前的状态乘以一个系数，记住一些当前的信息。</p><p>最后，我们应该决定一个节点的输出结果了。这个就诶过取决于我们的细胞状态，但是是过滤后的版本。<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/output.png"></p><p>我们先使用sigmoid层得到的我们应该输出的东西，然后哦将细胞状态通过tanh得到系数，将两个相乘，得到当前的状态h_t</p><p>LSTM有多个变种，这里就跳过。</p><p>其中比较重要的是BiLSTM,就是两个方向的LSTM的输出状态拼接到一起:<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/BiLSTM.png"></p><p>最后补充一下，在写代码的时候，使用的torch.nn.LSTM的官方资料说明:<br><img src="/2020/12/15/LSTM%E5%88%86%E6%9E%90/torch_nn.png"></p><p>下面是参数的说明:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input_size – The number of expected features <span class="keyword">in</span> the input x</span><br><span class="line"></span><br><span class="line">hidden_size – The number of features <span class="keyword">in</span> the hidden state h</span><br><span class="line"></span><br><span class="line">num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking <span class="keyword">in</span> outputs of the first LSTM and computing the final results. Default: 1</span><br><span class="line"></span><br><span class="line">bias – If False, <span class="keyword">then</span> the layer does not use bias weights b_ih and b_hh. Default: True</span><br><span class="line"></span><br><span class="line">batch_first – If True, <span class="keyword">then</span> the input and output tensors are provided as (batch, seq, feature). Default: False</span><br><span class="line"></span><br><span class="line">dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0</span><br><span class="line"></span><br><span class="line">bidirectional – If True, becomes a bidirectional LSTM. Default: False</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;关于LSTM模型:&lt;br&gt;&lt;img src=&quot;/2020/12/15/LSTM%E5%88%86%E6%9E%90/LSTM%E6%A8%A1%E5%9E%8B.png&quot;&gt;&lt;br&gt;参考:&lt;a href=&quot;https://colah.github.io/posts/2015-</summary>
      
    
    
    
    
    <category term="lstm" scheme="http://example.com/tags/lstm/"/>
    
  </entry>
  
  <entry>
    <title>spo抽取知识图谱</title>
    <link href="http://example.com/2020/12/11/spo%E6%8A%BD%E5%8F%96%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <id>http://example.com/2020/12/11/spo%E6%8A%BD%E5%8F%96%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</id>
    <published>2020-12-11T11:07:43.000Z</published>
    <updated>2020-12-11T11:33:04.815Z</updated>
    
    <content type="html"><![CDATA[<p>参考:<br><a href="https://github.com/percent4/spo_extract_platform">https://github.com/percent4/spo_extract_platform</a></p><p>知识图谱的构建由两个部分构成:<br>1.SPO三元组抽取:序列标注算法(ALBERT+BiLSTM+CRF)<br>SPO:Subject(主语) Predicate(谓语) Object(宾语)<br>sequence_labeling F1:81%</p><p>2.关系抽取:文本二分类(ALBERT+BiGRU+ATT)<br>text_classification F1:96%</p><p>提取无结构文本的应用在extract_example<br>下面会对这两个部分分别做完整的介绍:</p><h1 id="1-SPO三元组"><a href="#1-SPO三元组" class="headerlink" title="1.SPO三元组"></a>1.SPO三元组</h1><p><img src="/2020/12/11/spo%E6%8A%BD%E5%8F%96%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/ALBERT_BiLSTM_CRF.png"><br>上面是模型的框架</p><h2 id="1-ALBERT层"><a href="#1-ALBERT层" class="headerlink" title="1.ALBERT层"></a>1.ALBERT层</h2><p>albert是以单个汉字作为输入的(本次配置最大为128个，短句做padding)，两边分别加上开始标识CLS和结束标识SEP，输出的是每个输入word的embedding。在该框架中其实主要就是利用了预训练模型albert的词嵌入功能，在此基础上fine-tuning其后面的连接参数，也就是albert内部的训练参数不参与训练。</p><h2 id="2-BiLSTM层"><a href="#2-BiLSTM层" class="headerlink" title="2.BiLSTM层"></a>2.BiLSTM层</h2><p>该层的输入是albert的embedding输出，一般中间会加个project_layer，保证其输出是[batch_szie,num_steps, num_tags]。batch_size为模型当中batch的大小，num_steps为输入句子的长度，本次配置为最大128，num_tags为序列标注的个数，如图中的序列标注一共是5个，也就是会输出每个词在5个tag上的分数，由于没有做softmax归一化，所以不能称之为概率值。</p><h2 id="3-CRF层"><a href="#3-CRF层" class="headerlink" title="3.CRF层"></a>3.CRF层</h2><p>如果没有CRF层，直接按BiLSTM每个词在5个tag的最大分数作为输出的话，可能会出现【B-Person，O，I-Person，O，I-Location】这种序列，显然不符合实际情况。CRF层可以加入一些约束条件，从而保证最终预测结果是有效的。</p><p>例如：<br>句子的开头应该是“B-”或“O”，而不是“I-”。</p><h1 id="2-关系抽取"><a href="#2-关系抽取" class="headerlink" title="2.关系抽取"></a>2.关系抽取</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考:&lt;br&gt;&lt;a href=&quot;https://github.com/percent4/spo_extract_platform&quot;&gt;https://github.com/percent4/spo_extract_platform&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知识图谱的构建由两个部</summary>
      
    
    
    
    
    <category term="知识图谱" scheme="http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://example.com/2020/12/10/test/"/>
    <id>http://example.com/2020/12/10/test/</id>
    <published>2020-12-10T05:35:43.000Z</published>
    <updated>2020-12-10T05:38:27.776Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/12/10/test/cat.jpeg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/12/10/test/cat.jpeg&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Ubuntu之GPU运算不归路</title>
    <link href="http://example.com/2020/12/01/Ubuntu%E4%B9%8BGPU%E8%BF%90%E7%AE%97%E4%B8%8D%E5%BD%92%E8%B7%AF/"/>
    <id>http://example.com/2020/12/01/Ubuntu%E4%B9%8BGPU%E8%BF%90%E7%AE%97%E4%B8%8D%E5%BD%92%E8%B7%AF/</id>
    <published>2020-12-01T08:12:52.000Z</published>
    <updated>2020-12-01T08:22:05.332Z</updated>
    
    <content type="html"><![CDATA[<p>之前，本人尝试在Ubuntu系统上进行GPU运算，装GPU驱动等等。<br>以失败告终，最后，在命令行模式下转移了自己的部分资料，然后重装了系统。<br>面对这个惨痛的教训，我怎么能够忍气吞声，开始筹备第二次GPU远征。</p><p>这次，我先保存资料，以供后人借鉴。</p><p>首先，检查电脑上的GPU信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia <span class="comment">#可以查询所有的nvidia显卡</span></span><br><span class="line"></span><br><span class="line">lspci -v -s [显卡编号] <span class="comment">#可以查询显卡的具体属性</span></span><br><span class="line"></span><br><span class="line">nvidia-smi <span class="comment">#可以查看显卡的显存利用率，特别注意，这个需要下载驱动，没有下载驱动的，请小心，上次我就下载驱动之后就出问题了</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前，本人尝试在Ubuntu系统上进行GPU运算，装GPU驱动等等。&lt;br&gt;以失败告终，最后，在命令行模式下转移了自己的部分资料，然后重装了系统。&lt;br&gt;面对这个惨痛的教训，我怎么能够忍气吞声，开始筹备第二次GPU远征。&lt;/p&gt;
&lt;p&gt;这次，我先保存资料，以供后人借鉴。&lt;/</summary>
      
    
    
    
    
    <category term="ubuntu" scheme="http://example.com/tags/ubuntu/"/>
    
    <category term="gpu" scheme="http://example.com/tags/gpu/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用说明</title>
    <link href="http://example.com/2020/11/27/PyTorch%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://example.com/2020/11/27/PyTorch%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</id>
    <published>2020-11-27T11:25:15.000Z</published>
    <updated>2020-12-10T05:55:07.589Z</updated>
    
    <content type="html"><![CDATA[<p>参考资料:<br><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a></p><p>第一部分 tensor + operator</p><p>第二部分 autograd<br>torch.Tnesor是包中的核心类，<br>.requires_grad = True，那么会自动跟踪所有运算，<br>.backward() 当你结束运算调用这个函数，就会自动计算梯度。<br>.grad 梯度会计算放到这里</p><p>.detach() 可以停止跟踪<br>with torch.no_grad(): 停止跟踪<br>上面的在评估模型的时候非常有用</p><p>Function类也是一个很重要的类别。<br>.grad_fn 是一个tensor被一个函数建立</p><p>随机数初始化:<br>在神经网络中，参数默认是进行随机初始化的，不同的初始化参数往往会导致不同的结果，当得到比较好的结果的时候，我们希望结果可以复现，在torch中，通过设置随机数种子可以达到这个目的:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def set_seed(seed):</span><br><span class="line">    torch.manual_seed(seed)  </span><br><span class="line">    <span class="comment"># cpu 为CPU设置种子用于生成随机数，以使得结果是确定的</span></span><br><span class="line">    torch.cuda.manual_seed(seed)  </span><br><span class="line">    <span class="comment"># gpu 为当前GPU设置随机种子</span></span><br><span class="line">    torch.backends.cudnn.deterministic = True  </span><br><span class="line">    <span class="comment"># cudnn 每次返回的卷积算法都是确定的，即默认算法</span></span><br><span class="line">    np.random.seed(seed)  </span><br><span class="line">    <span class="comment"># numpy</span></span><br><span class="line">    random.seed(seed)  </span><br><span class="line">    <span class="comment"># random and transforms</span></span><br></pre></td></tr></table></figure><p>在程序的入口处设置随机数种子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set_seed(1)</span><br></pre></td></tr></table></figure><p>1.当我们训练了一个模型之后，需要将训练得到的模型进行保存。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;./cifar_net.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(),PATH)</span><br></pre></td></tr></table></figure><p>2.测试网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新加载保存的模型</span></span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure><p>保存模型的方式不同。会导致模型有可能不同</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(net,<span class="string">&#x27;./model.pth&#x27;</span>)</span><br><span class="line">torch.save(net.state_dict(),<span class="string">&#x27;./model-dict.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">net=torch.load(<span class="string">&#x27;./model.pth&#x27;</span>)</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;./model-dict.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了加载之后相同，需要指定eval模式</span></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">net=net.eval()</span><br><span class="line">torch.save(net,<span class="string">&#x27;./model.pth&#x27;</span>)</span><br><span class="line">torch.save(net.state_dict(),<span class="string">&#x27;./model-dict.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载</span></span><br><span class="line">net_load1=torch.load(<span class="string">&#x27;./model.pth&#x27;</span>)</span><br><span class="line">net_load1=net_load1.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">net_load2.load_state_dict(torch.load(<span class="string">&#x27;./model-dict.pth&#x27;</span>))</span><br><span class="line">net_load2=net_load2.eval()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>model.state_dict()是浅拷贝，返回的参数依然会随着网络的训练而变化，需要deepcopy或者拷贝到硬盘中。</p><p>在state_dict中有下面四个内容:<br>1._paramters<br>nn.parameter.Paramter，也就是组成Module的参数。例如一个nn.Linear通常由weight和bias参数组成。它的特点是默认requires_grad=True,也就是说训练过程中需要反向传播的<br>2._buffers<br>不需要参与反向传播的参数<br>3._modules<br>torch.nn.Module类，你定义的所有网络结构都必须继承这个类。<br>4._state_dict_hooks<br>最后一种就是在读取state_dict时希望执行的操作，一般为空，所以不做考虑。</p><p>关于NLLLoss的代码说明:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">torch.manual_seed(2019)</span><br><span class="line"></span><br><span class="line">output = torch.randn(1, 3)  <span class="comment"># 网络输出</span></span><br><span class="line">target = torch.ones(1, dtype=torch.long).random_(3)  <span class="comment"># 真实标签</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接调用</span></span><br><span class="line">loss = F.nll_loss(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化类</span></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><p>计算公式：loss(input, class) = -input[class]<br>公式理解：input = [-0.1187, 0.2110, 0.7463]，target = [1]，那么 loss = -0.2110<br>个人理解：感觉像是把 target 转换成 one-hot 编码，然后与 input 点乘得到的结果</p><p>如果 input 维度为 M x N，那么 loss 默认取 M 个 loss 的平均值，reduction=’none’ 表示显示全部 loss</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">torch.manual_seed(2019)</span><br><span class="line"></span><br><span class="line">output = torch.randn(2, 3)  <span class="comment"># 网路输出</span></span><br><span class="line">target = torch.ones(2, dtype=torch.long).random_(3)  <span class="comment"># 真实标签</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接调用</span></span><br><span class="line">loss = F.nll_loss(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化类</span></span><br><span class="line">criterion = nn.NLLLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">tensor([[-0.1187,  0.2110,  0.7463],</span></span><br><span class="line"><span class="string">        [-0.6136, -0.1186,  1.5565]])</span></span><br><span class="line"><span class="string">tensor([2, 0])</span></span><br><span class="line"><span class="string">tensor(-0.0664)</span></span><br><span class="line"><span class="string">tensor([-0.7463,  0.6136])</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>那么CrossEntropyLoss和NLLLoss区别在于<br>CrossEntropyLoss = Softmax+Log+NLLLoss</p><p>还有<br>ignore_index 就是计算的时候忽略的标签的数值</p><h1 id="梯度计算以及backward方法"><a href="#梯度计算以及backward方法" class="headerlink" title="梯度计算以及backward方法"></a>梯度计算以及backward方法</h1><p>tensor在torch中是一个n维数组，我们通过指定参数requires_grad = True来建立一个反向传播图，从而可以计算梯度。被称之为动态计算图Dynamic Computation Graph。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment"># solution 1</span></span><br><span class="line">x = torch.randn(2,2,requires_grad=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># solution 2</span></span><br><span class="line">x = torch.autograd.Variable(torch.Tensor([2,3]),requires_grad=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># solution 3</span></span><br><span class="line">x = torch.tensor([2,3],requires_grad=True,dtype=torch.float64)</span><br><span class="line"></span><br><span class="line"><span class="comment"># solution 4</span></span><br><span class="line">x = np.array([1,2,3],dtype=np.float64)</span><br><span class="line">x = torch.from_numpy(x)</span><br><span class="line">x.requires_grad = True</span><br><span class="line"><span class="comment"># or x.requires_grad(Ture)</span></span><br></pre></td></tr></table></figure><p>attention:<br>1.只有浮点型数据才有梯度，</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考资料:&lt;br&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;https://pytorch.org/tutorials/beginner/deep_lear</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="torch" scheme="http://example.com/tags/torch/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX使用说明</title>
    <link href="http://example.com/2020/11/26/LaTeX%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://example.com/2020/11/26/LaTeX%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</id>
    <published>2020-11-26T15:36:25.000Z</published>
    <updated>2020-11-26T15:37:44.914Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-环境配置"><a href="#0-环境配置" class="headerlink" title="0.环境配置"></a>0.环境配置</h1><p>下载</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-环境配置&quot;&gt;&lt;a href=&quot;#0-环境配置&quot; class=&quot;headerlink&quot; title=&quot;0.环境配置&quot;&gt;&lt;/a&gt;0.环境配置&lt;/h1&gt;&lt;p&gt;下载&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="latex" scheme="http://example.com/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱融合方法</title>
    <link href="http://example.com/2020/11/26/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2020/11/26/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95/</id>
    <published>2020-11-26T07:40:33.000Z</published>
    <updated>2020-12-11T11:02:02.227Z</updated>
    
    <content type="html"><![CDATA[<p>参考资料：<br>南京大学 CCF学科前沿讲习班 108期<br><a href="https://github.com/nju-websoft/KnowledgeGraphFusion">https://github.com/nju-websoft/KnowledgeGraphFusion</a></p><p>结构：<br>1.概述<br>2.预备知识<br>3.本体匹配<br>4.实体对齐<br>5.知识融合<br>6.总结与展望</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考资料：&lt;br&gt;南京大学 CCF学科前沿讲习班 108期&lt;br&gt;&lt;a href=&quot;https://github.com/nju-websoft/KnowledgeGraphFusion&quot;&gt;https://github.com/nju-websoft/KnowledgeGr</summary>
      
    
    
    
    
    <category term="知识图谱" scheme="http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Python模块打包及发布</title>
    <link href="http://example.com/2020/11/24/Python%E6%A8%A1%E5%9D%97%E6%89%93%E5%8C%85%E5%8F%8A%E5%8F%91%E5%B8%83/"/>
    <id>http://example.com/2020/11/24/Python%E6%A8%A1%E5%9D%97%E6%89%93%E5%8C%85%E5%8F%8A%E5%8F%91%E5%B8%83/</id>
    <published>2020-11-24T11:19:57.000Z</published>
    <updated>2020-11-25T03:16:10.105Z</updated>
    
    <content type="html"><![CDATA[<p>概述：<br>如何将自己开发的库分享给别人使用，使用pip install安装。<br>本文章包含制作python安装包和发布。<br>并且，通过这个步骤，我可深入了解各个模块中的内容，方便自己使用别人的库</p><p>参考资料:<br><a href="https://packaging.python.org/tutorials/packaging-projects/">https://packaging.python.org/tutorials/packaging-projects/</a></p><h1 id="1-创建setup-py文件"><a href="#1-创建setup-py文件" class="headerlink" title="1.创建setup.py文件"></a>1.创建setup.py文件</h1><p>建立如下所示的文档结构</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── example_pkg <span class="comment"># packet 文件</span></span><br><span class="line">│   ├── __init__.py <span class="comment"># 全局变量定义等，可以直接是空文件，定位</span></span><br><span class="line">│   └── test_pkg.py </span><br><span class="line">├── LICENSE <span class="comment"># 许可证</span></span><br><span class="line">├── README.md <span class="comment"># 描述文件</span></span><br><span class="line">├── setup.py <span class="comment"># 配置文件</span></span><br><span class="line">└── tests <span class="comment"># 测试文件夹</span></span><br></pre></td></tr></table></figure><p>其中setup.py文件里面需要设置packet的配置,里面有注释：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import setuptools</span><br><span class="line"></span><br><span class="line">with open(<span class="string">&quot;README.md&quot;</span>,<span class="string">&quot;r&quot;</span>) as fh:</span><br><span class="line">    long_description = fh.read()</span><br><span class="line"></span><br><span class="line">setuptools.setup(</span><br><span class="line">    name = <span class="string">&quot;Emir-Liu-packet&quot;</span>, <span class="comment">#distribution name of package</span></span><br><span class="line">    version = <span class="string">&quot;0.0.1&quot;</span>,</span><br><span class="line">    author = <span class="string">&quot;Emir-Liu&quot;</span>,</span><br><span class="line">    author_email = <span class="string">&quot;egliuym@gmail.com&quot;</span>,</span><br><span class="line">    description = <span class="string">&quot;my first test packet&quot;</span>, <span class="comment"># a short,one-sentence summary</span></span><br><span class="line">    long_description = long_description, <span class="comment"># the long description is loaded from README.md</span></span><br><span class="line">    long_description_content_type = <span class="string">&quot;text/markdown&quot;</span>,</span><br><span class="line">    url = <span class="string">&quot;https://emir-liu.github.io/&quot;</span>, <span class="comment"># URL of the homepage of the project</span></span><br><span class="line">    packages = setuptools.find_packages(), <span class="comment"># a list of all python import packages that should be included in the destribution package</span></span><br><span class="line">    classifiers = [</span><br><span class="line">        <span class="string">&quot;Programming Language :: Python :: 3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;License :: OSI Approved :: MIT License&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Operating System :: OS Independent&quot;</span>,</span><br><span class="line">    ], <span class="comment"># some additional metadata about package ,at least license ,operating system and verson of python</span></span><br><span class="line">    python_requires = <span class="string">&#x27;&gt;=3.0&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="2-打包文件"><a href="#2-打包文件" class="headerlink" title="2.打包文件"></a>2.打包文件</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install --user --upgrade setuptools wheel</span><br><span class="line">python3 setup.py sdist bdist_wheel</span><br></pre></td></tr></table></figure><p>注意打包文件里面不写程序是没有输出的，打包之后应该会有文件夹（dist中包含.whl和.tar.gz文件），之后上传这个文件夹</p><h1 id="3-注册Pypi-然后上传"><a href="#3-注册Pypi-然后上传" class="headerlink" title="3.注册Pypi,然后上传"></a>3.注册Pypi,然后上传</h1><p>先注册pypi,邮箱激活</p><p>然后，在~/.pypirc中写入:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[distutils]</span><br><span class="line">index-servers = </span><br><span class="line">    pypi</span><br><span class="line">    testpypi</span><br><span class="line"></span><br><span class="line">[testpypi]</span><br><span class="line">repository = https://test.pypi.org/legacy/</span><br><span class="line"></span><br><span class="line">[pypi]</span><br><span class="line">repository = https://upload.pypi.org/legacy/</span><br><span class="line">username = xxx</span><br><span class="line">password = xxx</span><br></pre></td></tr></table></figure><p>这一步只要一开始弄好就不用输入密码什么的了。</p><p>上传：<br>twine upload dist/*</p><p>之后你可以在网站上看到你的包了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;概述：&lt;br&gt;如何将自己开发的库分享给别人使用，使用pip install安装。&lt;br&gt;本文章包含制作python安装包和发布。&lt;br&gt;并且，通过这个步骤，我可深入了解各个模块中的内容，方便自己使用别人的库&lt;/p&gt;
&lt;p&gt;参考资料:&lt;br&gt;&lt;a href=&quot;https://</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱融合</title>
    <link href="http://example.com/2020/11/23/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%9E%8D%E5%90%88/"/>
    <id>http://example.com/2020/11/23/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%9E%8D%E5%90%88/</id>
    <published>2020-11-23T01:42:20.000Z</published>
    <updated>2020-11-23T10:27:15.448Z</updated>
    
    <content type="html"><![CDATA[<p>将知识图谱进行总结：<br>概念图谱、实体概念混合图谱、素材文档图谱+工作文档图谱</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">构造|概念&lt;-混合|混合&lt;-素材+工作文档</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">节点|人工补全(略)|自动去重（2）</span><br><span class="line">|冲突检测|自动合并（2）</span><br><span class="line">||冲突检测</span><br><span class="line">||自动挂接（2）</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">条目|生成编辑|自动合并（2）</span><br><span class="line">||编辑合并</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">概念(概念图谱)|归纳聚类（1）|X</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">领域(混合图谱)|X|领域相似性检测</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">子图(混合图谱)|X|子图分类（1）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>冲突检测：<br>获取知识图谱的领域树；获取知识图谱中的当前实体；获得当前实体在领域树中所属的第一领域及第二领域；分别确定第一领域及第二领域在领域树中的位置；根据第一领域及第二领域在领域树中的位置，确定当前实体的第一领域及第二领域的关系；确定第一领域及第二领域在领域树中不存在公共子领域，或者，确定第一领域及第二领域在领域树中存在公共父领域且公共父领域为通用领域，将第一领域及第二领域确定为可疑冲突领域对；根据可疑冲突领域对，获得知识图谱的冲突领域对。该方法可以得到知识图谱中存在的冲突领域对，覆盖率很高。</p><p>冲突检测和冲突消解：<br>将节点之间的冲突分为:<br>术语冲突：实体冲突<br>谓词冲突：表达相同知识采用不同形式的谓词<br>语义冲突：多个知识在逻辑上产生的不一致</p><p>提出了<br>逻辑树融合法:术语冲突<br>频率融合法:术语冲突、谓词冲突<br>句法融合法:术语冲突、谓词冲突和语义冲突</p><p>参考：<br>知识图谱实体领域冲突检测方法,装置及相关设备？？<br>申请(专利权)人：腾讯科技（深圳）有限公司</p><p>1.分类/聚类问题：</p><p>子图分类:<br>介绍：<br>找到同构子图<br>算法：GNN/GCN<br>简介：<br>训练一个GCN网络，将多个KG的实体和关系都映射到同一个空间，相同实体对和关系对有相同的向量表示，然后根据空间中向量的相似性来寻找对齐的实体。</p><p>参考：<br>基于图神经网络的知识图谱研究进展<br>融合多个知识图谱形成一个更完整的知识图谱。由于图神经网络具有识别同构子图的能力，而可对齐的实体对周围通常有相似的邻居，即具有一定的同构特征，因此目前有许多研究者尝试将图神经网络用于实体对齐。<br>Multi-Channel Graph Neural Network for Entity Alignment<br><a href="https://arxiv.org/pdf/1908.09898v1.pdf">https://arxiv.org/pdf/1908.09898v1.pdf</a><br>该方法的缺点：<br>1.结构异质性：不同KG会有不同的结构，从而根本就无法对齐。<br>2.有限的种子对</p><p>概念归纳聚类:<br>介绍：<br>谱聚类，图中不同点进行聚类，将图进行分割。<br>算法：RatioCut/Ncut切图+k-means聚类<br>简介：<br>选一个cut(a1,a2,..,an).然后k-means.</p><p>参考:<br><a href="https://www.cnblogs.com/pinard/p/6221564.html">https://www.cnblogs.com/pinard/p/6221564.html</a></p><p>自动去重/合并：<br>比较实体的语义和结构特征，找同义词<br>算法：<br>对非结构化数据做语义分析，找到同义词对<br>1） 选取一些特定分词结果做同义词挖掘。如果需要考虑语料中可能出现的新词或者不同语言表述，则需要配合Pattern挖掘、NER或名词短语抽取等方式获取候选词。<br>2） 准备好已有的同义词表作为种子数据<br>3） 获取所有种子词和候选词的特征，通常该任务的特征会从两个角度考虑，分别是local context和global context，通俗的讲就是局部特征和全局特征，前者着重于词本身，常见字级别特征、词级别特征等；后者则是考虑目标词在数据集中的分布特征或者词所在句子、段落的语义特征</p><p>参考：<br><a href="https://blog.csdn.net/jxsdq/article/details/106002991">https://blog.csdn.net/jxsdq/article/details/106002991</a><br><a href="https://zhuanlan.zhihu.com/p/105203565">https://zhuanlan.zhihu.com/p/105203565</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;将知识图谱进行总结：&lt;br&gt;概念图谱、实体概念混合图谱、素材文档图谱+工作文档图谱&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa</summary>
      
    
    
    
    
    <category term="知识图谱" scheme="http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Schedule</title>
    <link href="http://example.com/2020/11/23/Schedule/"/>
    <id>http://example.com/2020/11/23/Schedule/</id>
    <published>2020-11-22T17:50:27.000Z</published>
    <updated>2020-11-24T08:25:03.193Z</updated>
    
    <content type="html"><![CDATA[<p>ing:<br>1.bert </p><p>frame:</p><p>neural network<br>convolution neural network</p><p>rnn<br>lstm<br>transformers<br>bert<br>gpt-2</p><p>lecture:</p><p>Deep learning for human language processing</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ing:&lt;br&gt;1.bert &lt;/p&gt;
&lt;p&gt;frame:&lt;/p&gt;
&lt;p&gt;neural network&lt;br&gt;convolution neural network&lt;/p&gt;
&lt;p&gt;rnn&lt;br&gt;lstm&lt;br&gt;transformers&lt;br&gt;bert&lt;br&gt;gpt-2&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="schedule" scheme="http://example.com/tags/schedule/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2020/11/18/hello-world/"/>
    <id>http://example.com/2020/11/18/hello-world/</id>
    <published>2020-11-18T13:05:09.561Z</published>
    <updated>2020-11-18T13:25:41.988Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AdaBoost模型</title>
    <link href="http://example.com/2020/10/23/AdaBoost%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2020/10/23/AdaBoost%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-10-23T11:56:51.000Z</published>
    <updated>2020-11-18T13:25:41.940Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>资料来源:<br><a href="https://blog.paperspace.com/adaboost-optimizer/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones">https://blog.paperspace.com/adaboost-optimizer/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones</a>.</p><p>Ensemble Learning:<br>集成学习(或者是组合学习)，将多个基础的算法相结合，构造成一个优化的预测模型。</p><p>为什么需要集成学习:<br>1.减少变量<br>2.减少偏置<br>3.提高预测性能</p><p>集成学习被分为了两个类型:</p><ol><li>顺序学习，通过依次生成不同的模型，之前模型的错误被后来的模型学习，利用模型之间的依赖性，使得错误标记有更大的权重。</li></ol><p>例如:AdaBoost</p><p>2.并行学习，并行产生基本的模型，这通过平均错误来利用了模型的独立性。</p><p>例如:Random Forest 随机森林</p><p>集成方法中的Boosting<br>例如人类从错误中学习，并且尝试将来不再犯同样的错误，Boost算法尝试从多个弱分类器的错误中建立一个更强的学习预测模型。<br>你一开始从训练数据中建立一个模型，<br>然后你从上一个模型中建立第二个模型通过尝试从减少上一个模型的误差。<br>模型被顺序增加，每一个都修改它的前面一个模型，知道训练数据被完美的预测或者增加了最大数量的模型。</p><p>Boosting基本上是试图减少模型无法识别数据中的相关趋势的时候所引起的偏差。这是通过评估预测数据和实际数据的差值所实现的。</p><p>Boost算法的类别:<br>1.AdaBoost(Adaptive Boosting)<br>2.Gradient Tree Boosting<br>3.XGBoost</p><p>我们主要注意AdaBoost中的细节，这个是Boost算法中最流行的算法。</p><p>单个分类器也许无法正确预测每个对象的类别，但是当我们将多个弱分类器组合到一起，每个分类器从其他分类器中分类错误的对象中进行学习，我们可以构建一个强模型，弱分类器可以是任何基本分类器，从决策树到Logistic回归等。</p><p>那么什么是弱分类器？<br>弱分类器的性能优于随机猜测，但是在指定对象的类中表现不佳，例如，较弱的分类器可以预测40岁以上的每个人都无法参加马拉松比赛，但是低于40岁的人可以参加马拉松比赛，也许就会有60%以上的准确度，但是仍然会有许多数据点误分。</p><p>AdaBoost不仅仅可以作为模型本身使用，还可以应用在任何分类器之上，从其缺点中学习并且提出更加准确的模型。因此可以称为最佳现成分类器。</p><p>那么开始了解AdaBoost是如何和决策树桩一起使用，决策树桩就像随机森林中的数目，但不是完全生长的，它有一个节点和两个叶子，AdaBoost使用了这样的树桩森林，而不是树。</p><p>单靠树桩并不是做出决定的好方法。一棵成熟的树结合了所有变量的决策以预测目标值。另一方面，树桩只能使用一个变量来做出决定。让我们通过查看几个变量来确定一个人是否“健康”（身体健康），来逐步了解AdaBoost算法的幕后知识。</p><p>例子:<br>STEP1:<br>一个弱分类，基于加权的样本，样本的权重表示正确分类的重要性，对于第一个弱分类，我们给所有样本同样的权重。</p><p>STEP2:<br>我们为每一个变量创造一个决策树桩，并查看每一个树桩分类的结果，例如，我们检查了年龄、吃垃圾食物和锻炼。我们将看每个树桩将多少样本正确分类。</p><p>STEP3:<br>我们将更多的权重分配给分类错误的样本，以便在下一个决策树桩中对样本进行正确的分类，权重还会根据每个分类器的准确性分配给每个分类器，高精度的分类器就有更好的权重。</p><p>STEP4:<br>第二步重复进行，直到所有数据点都正确分类。</p><p><img src="fully_grown_tree_vs_stumps.png"></p><p><img src="more_import_stumps.jpeg"></p><p>下面是数学方面的表示:<br>首先是数据集:<br><img src="input.png"><br>其中:<br>n:数字的维度，或者说是数据集的特征数</p><p>x:数据点的集合</p><p>y:目标变量为-1和1,当为二分类问题的时候</p><p>那么，每个样本的权重，如何计算?</p><p>一开始的时候，每个样本的权重都相同，为1/N<br>其中:N为数据点的总数</p><p>这使得加权的样本和为1</p><p>之后，计算分类器的影响力，通过下面的公式:<br><img src="classifier_inf.png"></p><p>Total Error就是错误分类的数目，那么关系如下图所示:<br><img src="Alpha_vs_Error_Rate.png"></p><p>记住，当分类器表现好的时候，影响力大，当0.5的时候为0,当非常不好的时候，为负数。</p><p>之后，样本的权重呢?<br><img src="sample_weights.png"></p><p>总结:<br>AdaBoost的好坏:<br>优点：<br>比SVM简单好用，不需要调整参数<br>AdaBoost不会倾向于过拟合<br>提高了弱分类器的准确性，使其更加灵活</p><p>缺点：<br>需要有高质量的数据，对噪声敏感</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;资料来源:&lt;br&gt;&lt;a href=&quot;https://blog.paperspace.com/adaboost-optimizer/#:~:t</summary>
      
    
    
    
    
    <category term="adaboost" scheme="http://example.com/tags/adaboost/"/>
    
  </entry>
  
  <entry>
    <title>RNN神经网络</title>
    <link href="http://example.com/2020/10/18/RNN%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2020/10/18/RNN%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2020-10-18T15:07:02.000Z</published>
    <updated>2020-11-18T13:25:41.992Z</updated>
    
    <content type="html"><![CDATA[<p><img src="RNN%E6%A8%A1%E5%9E%8B.jpg"><br>RNN背后的想法是利用顺序信息。在传统的神经网络中，我们假设所有输入（和输出）彼此独立。但是对于许多任务来说，这是一个非常糟糕的主意。如果要预测句子中的下一个单词，则最好知道哪个单词在它之前。RNN之所以称为递归， 是因为它们对序列的每个元素执行相同的任务，其输出取决于先前的计算。思考RNN的另一种方法是，它们具有“内存”，可以捕获有关到目前为止已计算出的内容的信息。从理论上讲，RNN可以任意长的顺序使用信息，但实际上，它们仅限于回顾一些步骤（稍后再介绍）。</p><p>通过展开，我们仅表示我们为整个序列写出了网络。例如，如果我们关心的序列是5个单词的句子，则该网络将展开为5层神经网络，每个单词一层。控制RNN中发生的计算的公式如下：</p><p>￼是时间步的输入￼。例如，￼ 可以是与句子的第二个单词相对应的“一字通”向量。<br>￼是时间步的隐藏状态￼。这是网络的“内存”。￼根据先前的隐藏状态和当前步骤的输入来计算￼。该函数￼通常是诸如tanh或ReLU之类的非线性函数。  ￼计算第一个隐藏状态所需的，通常初始化为全零。</p><p>下面的内容是如何实现RNN网络:<br><a href="https://blog.csdn.net/rtygbwwwerr/article/details/51012699">https://blog.csdn.net/rtygbwwwerr/article/details/51012699</a><br><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/</a><br>语言模型:<br>某一句话s是由n个词语(w)组成的集合，那么这句话生成的概率为:<br>P(s) = P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)…P(w_n|w_1,…,w_n-1)</p><p>公式中，需要n个联合条件分布率，通常我们做一些假设，每一个词语都依赖它的前一个词语，那么就可以化简为:<br>P(s) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_n|w_n-1)<br>当依赖两个单词的时候同样类似。</p><p>上面就被称为2-Gram和3-Gram,关联的词语越多，模型就越精确，训练成本也很大。</p><p>这里使用RNN训练一个2-Gram模型。也就是通过RNN来估计每对单词的条件概率P(w_i|w_j)</p><p>训练样本数为Ns,目标词典的word数量为Nw,每个单词使用one-hot表示。<br>输入/出层节点数为 Ni=No=Nw<br>隐藏层节点数为Nh(通常为Ni的十分之一)<br>隐藏层的激活函数为tanh,<br>输出层激活函数为softmax<br>隐藏层存在一个递归边，每个时刻的输出乘以权值矩阵W后，将作为下一个时刻输入的一部分。</p><p>权值矩阵:<br>U :Input Layer      -&gt; Hidden Layer Nh<em>Ni<br>V :Hidden(t) Layer  -&gt; Output Layer No</em>Nh<br>W :Hidden(t-1) Layer-&gt; Hidden(t) Layer Nh*Nh<br>f_h:隐藏层激活函数<br>f_o:输出层激活函数</p><p>s_k(t):t时刻第k个隐藏层节点的输出值<br>o_k(t):t时刻第k个输出层节点的输出值</p><p>y_k:第k个输出层就诶点的目标输出值</p><p><img src="RNN%E7%BB%93%E6%9E%84.png"></p><p>程序的组成:</p><p>1.标记文本：<br>我们有原始的文本，但是，我们希望在词语的基础上进行预测，这就表示，我们需要在句子中对文本进行标识，我们可以通过空格将每个评论分割开，但是这样就无法正确处理符号，<br>句子‘He left!’应该是三个词语令牌:’He’,’left’,’!’我们将会使用NLTK中的word_tokenize和sent_tokenize方法，这将会替我们完成最复杂的工作。</p><p>2.删除低频词语<br>文本中的大多数词语仅仅出现1到2次，将低频的词语删除是一个好方法。<br>大量的词汇表将会使得训练的速度降低。而且，由于我们没有很多低频词语的上下文实例，因此我们也无法学习如何正确使用低频词语。为了能够理解如何使用一个单词，你需要在不同的上下文中看到它。</p><p>代码中，我们将最常用的词汇表限制为vocabulary_size，默认为8000，但是可以随意修改。我们将不包含在词汇表中的词语用UNKNOWN_TOKEN来代替。</p><p>我们将会把UNKNOWN——TOKEN作为我们词汇的一部分，当我们生成新的文本的时候，我们可以再次替换UNKNOWN——TOKEN，例如通过在词汇表中抽取一个随机采样的单词，或者我们可以通过生成句子，直到得到一个不包含未知标记的句子。</p><p>3.添加开始和结束标记:<br>我们同样希望学习哪些词语在开头或者结尾，为此，我们准备了SENTENCE——START和SENTENCE-END令牌，考虑到第一个token是SENTENCE-START,那么什么词语跟在后面呢，也就是句子的第一个单词。</p><p>4.建立训练数据矩阵<br>CNN的输入是向量，我们需要建立一个单词到序列的映射，index_to_word和word_to_index</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;RNN%E6%A8%A1%E5%9E%8B.jpg&quot;&gt;&lt;br&gt;RNN背后的想法是利用顺序信息。在传统的神经网络中，我们假设所有输入（和输出）彼此独立。但是对于许多任务来说，这是一个非常糟糕的主意。如果要预测句子中的下一个单词，则最好知道哪个单词在它之前。</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>idea</title>
    <link href="http://example.com/2020/10/12/idea/"/>
    <id>http://example.com/2020/10/12/idea/</id>
    <published>2020-10-12T01:04:14.000Z</published>
    <updated>2020-12-16T03:47:45.977Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-概述"><a href="#0-概述" class="headerlink" title="0.概述"></a>0.概述</h1><p>学习过程中的小想法:</p><h1 id="1-内容"><a href="#1-内容" class="headerlink" title="1.内容"></a>1.内容</h1><p>adaboost: 样本权重通过聚合程度过滤之后再修改权重，避免误差</p><p>BERT+判断模型 NER ELECTRA</p><p>字图片+上下文 分析NER</p><p>BERT中究竟学了什么内容</p><p>spo 平台实现</p><p>NER博客流程</p><p>知识图谱</p><p>损失迭代，多任务</p><p>增加池化层，深度学习，度深学习</p><p>Enhanced Language Representation with information entities ERNIE 在pre train中增加其他信息</p><p>概率图</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-概述&quot;&gt;&lt;a href=&quot;#0-概述&quot; class=&quot;headerlink&quot; title=&quot;0.概述&quot;&gt;&lt;/a&gt;0.概述&lt;/h1&gt;&lt;p&gt;学习过程中的小想法:&lt;/p&gt;
&lt;h1 id=&quot;1-内容&quot;&gt;&lt;a href=&quot;#1-内容&quot; class=&quot;headerlink</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>知识图谱推理</title>
    <link href="http://example.com/2020/10/11/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86/"/>
    <id>http://example.com/2020/10/11/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86/</id>
    <published>2020-10-10T22:32:19.000Z</published>
    <updated>2020-11-18T13:25:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>综述参考:<br>文献[27]对知识抽取的研究进展进行了综述,<br>文献[28-31]分别对知识图谱构建、实体对齐、知识表示学习以及知识融合进行了综述,<br>文献[32]对知识抽取、知识表示、知识融合和知识推理这4个方面的研究进展进行了总结和展望.<br>面向知识图谱的知识推理研究进展</p><p>面向知识图谱的知识推理旨在根据已有的知识推理出新的知识或识别错误的知识</p><p>它包括两方面的内容:<br>知识图谱补全(knowledge graph completion,knowledge base completion)[1316]<br>包括:<br>连接预测 (link prediction)[19-21]<br>实体预测(entity prediction)[22-24]<br>关系预测(relation prediction)[22-25]<br>属性预测(attribute prediction)[26]等</p><p>知识图谱去噪 (knowledge graph refinement,knowledge graph cleaning)[17,18].</p><p>据推理类型划分,将面向知识图谱的知识推理分为单步推理和多步推理,<br>根据方法的不同,每类又包括基于规则的推理、基于分布式表示的推理、基于神经网络的推理以及混合推理.</p><p>目前的知识图谱有<br>KnowItAll[5]<br>YAGO[6-8]<br>DBpedia[9]<br>Freebase[10]、<br>NELL[11]<br>Probase[12]等.</p><p>人工智能下一个十年</p><p>知识图谱+认知推理+逻辑表达 -&gt; 认知图谱(Cognitive Graph)</p><p>深度学习算法经过的阶段:<br>1.向前网络:深度学习算法<br>2.自学习、自编码代表的学习时代<br>3.自循环神经网络，概率图模型<br>4.增强学习</p><p>路线:<br>1.脑科学<br>2.数据与知识</p><p>方向:<br>1.推理<br>2.可解释能力</p><p>paper: Das, R. , Neelakantan, A. , Belanger, D. , &amp; Mccallum, A. . (2016). Chains of reasoning over entities, relations, and text using recurrent neural networks</p><p>pro : <a href="https://rajarshd.github.io/ChainsofReasoning/">https://rajarshd.github.io/ChainsofReasoning/</a></p><p>OWL等规则及Jena工具</p><p>源码:<a href="https://github.com/debuluoyi">https://github.com/debuluoyi</a></p><p>基于基本的Path-RNN的架构进行改进。<br>基本Path-RNN:<br>输入:两个实体之间的路径，<br>输出:推理出的二者之间的新关系。<br>将关系之间的连接用RNN表示来进行推理。路径的表示是在处理完路径中所有的关系之后由RNN的最后的隐状态给出的。</p><p>改进有：<br>1.之前要为每一个需要预测的relation-type单独训练模型。而优化后只训练一个RNN来预测所有的relation type，共享了RNN参数精度也显著提高了。共享了relation type的表示以及RNN的composition matrices，这样同样的训练数据变量就大大减少了。训练模型的损失函数用的是negative log-likelihood<br>2.本文使用了neural attention机制对多条路径进行推理。之前的工作只推理了relation，没有推理组成路径上节点的entities，本文对关系类型，实体和实体类型进行了联合学习和推理。<br>3.分别用Top-kaverage和LogSumExp等多纬度为每一条路径的相似度评分加上权重，这样就考虑了每一条路径包含的信息，而不仅仅是评分最高的那条。</p><p>解决方法和公式:<br>首先进行下列的定义:<br>(e_s,e_t):一对实体<br>S:实体之间的路径集合<br>Pi = {e_s,r_1,e_1,r_2,…,r_k,e_t}:(e_s,e_t)之间的一条路径，<br>其中，路径的长度是路径上的关系的数目<br>len(Pi) = k</p><p>y_rt 表示节点r_t的向量表式，维度为d<br>RNN模型将所有的关系放到Pi序列中使用了一个RNN网络。中间层表示是h_t。</p><p><img src="RNN%E5%85%AC%E5%BC%8F1.png"></p><p>其中W_hh(h<em>h)和W_ih(d</em>h)分别是RNN网络的参数，其中上标r表示的是询问的关系。Path-RNN对于每一个关系都有不同的参数(y_rt,W_hh,W_ih)。<br>上面的f是sigmoid函数，路径的向量表式Pi(y_pi)是最后的隐藏层h_k.</p><p>y_pi和y_r的相似度可以计算为:<br><img src="%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97.png"></p><p>对于一个实体对的N条通路，Path-RNN通过下列的公式计算了两个实体之间的关系:<br><img src="PRNN%E5%A4%9A%E9%80%9A%E8%B7%AF.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;综述参考:&lt;br&gt;文献[27]对知识抽取的研究进展进行了综述,&lt;br&gt;文献[28-31]分别对知识图谱构建、实体对齐、知识表示学习以及知识融合进行了综述,&lt;br&gt;文献[32]对知识抽取、知识表示、知识融合和知识推理这4个方面的研究进展进行了总结和展望.&lt;br&gt;面向知识图谱的</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Python之正则表达式详解</title>
    <link href="http://example.com/2020/10/08/Python%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2020/10/08/Python%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%AF%A6%E8%A7%A3/</id>
    <published>2020-10-07T21:02:22.000Z</published>
    <updated>2020-11-18T13:25:41.988Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-概述"><a href="#0-概述" class="headerlink" title="0.概述"></a>0.概述</h1><p>资料来源:<br><a href="https://docs.python.org/zh-cn/3/library/re.html">https://docs.python.org/zh-cn/3/library/re.html</a></p><p>模式和被搜索的字符串可以是Unicode字符串(str)或者8位字节串(bytes).但是两者不能够混用，</p><p>提供正则表达式操作。</p><p>其中有常用的几种方法:<br>不进行转义处理: r”\n” 表示 \和n两个字符的字符串，而”\n”表示换行符。</p><p>有一些字符是特殊的元字符(matacharacters),并且不匹配自己。相反，它们表示匹配一些不同的东西，或者通过重复它们或者改变它们的含义来影响正则的其他部分。</p><h1 id="1-元字符的完整列表"><a href="#1-元字符的完整列表" class="headerlink" title="1.元字符的完整列表:"></a>1.元字符的完整列表:</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">. ^ $ * + ? &#123;  &#125; [  ] \ | (  )</span><br></pre></td></tr></table></figure><h2 id="1-1-匹配字符"><a href="#1-1-匹配字符" class="headerlink" title="1.1 匹配字符"></a>1.1 匹配字符</h2><p>[  ]:用来指定字符类，希望匹配的一组字符。<br>例如：<br> [abc] 将匹配任何字符 a、 b 或 c ；这与 [a-c] 相同，它使用一个范围来表示同一组字符。 如果你只想匹配小写字母，你的正则是 [a-z] 。</p><p> 字符类中的元字符不生效。<br> 例如：<br> [akm$] 将匹配 ‘a’ ， ‘k’ 、 ‘m’ 或 ‘$’ 中的任意字符； ‘$’ 通常是一个元字符，但在一个字符类中它被剥夺了特殊性。</p><p>你可以通过以下方式匹配 complementing 设置的字符类中未列出的字符。这通过包含一个 ‘^’ 作为该类的第一个字符来表示。 例如，[^5] 将匹配除 ‘5’ 之外的任何字符。 如果插入符出现在字符类的其他位置，则它没有特殊含义。 例如：[5^] 将匹配 ‘5’ 或 ‘^’。</p><p>也许最重要的元字符是反斜杠，\。 与 Python 字符串文字一样，反斜杠后面可以跟各种字符，以指示各种特殊序列。它也用于转义所有元字符，因此您仍然可以在模式中匹配它们；例如，如果你需要匹配 [ 或 \，你可以在它们前面加一个反斜杠来移除它们的特殊含义：[ 或 \。</p><p>一些以 ‘&#39; 开头的特殊序列表示通常有用的预定义字符集，例如数字集、字母集或任何非空格的集合。</p><p>\d<br>\D<br>\s<br>\S<br>\w<br>\W<br>.(点) 在默认模式，匹配除了换行的任意字符。如果指定了标签 DOTALL ，它将匹配包括换行符的任意字符。</p><p>^<br>(插入符号) 匹配字符串的开头，</p><h2 id="1-2-重复"><a href="#1-2-重复" class="headerlink" title="1.2 重复"></a>1.2 重复</h2><p>重复中我们要了解的第一个元字符是 *。 * 与字面字符 ‘*’ 不匹配；相反，它指定前一个字符可以匹配零次或多次，而不是恰好一次。</p><p>例如，ca*t 将匹配 ‘ct’ (0个 ‘a’ 字符)，’cat’ (1个 ‘a’ )， ‘caaat’ (3个 ‘a’ 字符)，等等。</p><p>另一个重复的元字符是 +，它匹配一次或多次。 要特别注意 * 和 + 之间的区别；* 匹配 零次 或更多次，因此重复的任何东西都可能根本不存在，而 + 至少需要 一次。 </p><p>例如，ca+t 将匹配 ‘cat’ (1 个 ‘a’)，’caaat’ (3 个 ‘a’)，但不会匹配 ‘ct’。</p><p>重复限定字符，问号字符 ? 匹配一次或零次；你可以把它想象成是可选的。</p><p>例如，home-?brew 匹配 ‘homebrew’ 或 ‘home-brew’。</p><p>最复杂的重复限定符是 {m,n}，其中 m 和 n 是十进制整数。 这个限定符意味着必须至少重复 m 次，最多重复 n 次。</p><p>例如，a/{1,3}b 将匹配 ‘a/b’ ，’a//b’ 和 ‘a///b’ 。 它不匹配没有斜线的 ‘ab’，或者有四个的 ‘a////b’。</p><p>你可以省略 m 或 n; 在这种情况下，将假定缺失值的合理值。 省略 m 被解释为 0 下限，而省略 n 则为无穷大的上限。</p><p>还原论者的读者可能会注意到其他三个限定符都可以用这种表示法表达。 {0,} 与 * 相同， {1,} 相当于 + ， {0,1} 和 ? 相同。 最好使用 * ， + 或 ? ，只要因为它们更短更容易阅读。</p><h1 id="2-使用正则表达式"><a href="#2-使用正则表达式" class="headerlink" title="2.使用正则表达式"></a>2.使用正则表达式</h1><p>方法 / 属性</p><p>目的</p><p>match()：确定正则是否从字符串的开头匹配。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">group()：返回正则匹配的字符串</span><br><span class="line">start()：返回匹配的开始位置</span><br><span class="line">end()：返回匹配的结束位置</span><br><span class="line">span()：返回包含匹配 (start, end) 位置的元组</span><br></pre></td></tr></table></figure><p>search()：扫描字符串，查找此正则匹配的任何位置。</p><p>findall()：找到正则匹配的所有子字符串，并将它们作为列表返回。</p><p>finditer()：找到正则匹配的所有子字符串，并将它们返回为一个 iterator。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-概述&quot;&gt;&lt;a href=&quot;#0-概述&quot; class=&quot;headerlink&quot; title=&quot;0.概述&quot;&gt;&lt;/a&gt;0.概述&lt;/h1&gt;&lt;p&gt;资料来源:&lt;br&gt;&lt;a href=&quot;https://docs.python.org/zh-cn/3/library/re.h</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="re" scheme="http://example.com/tags/re/"/>
    
  </entry>
  
</feed>
